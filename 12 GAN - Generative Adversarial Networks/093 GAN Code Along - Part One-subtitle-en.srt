1
00:00:05,320 --> 00:00:10,510
Welcome everyone to part one of our code along we're going to create a generative adversarial network

2
00:00:10,840 --> 00:00:13,580
to generate this data digits.

3
00:00:13,600 --> 00:00:16,100
Let's go ahead and open up a notebook and get started.

4
00:00:16,420 --> 00:00:23,620
All right let's start of our imports will do tensor flow of course as T.F. and then we're also going

5
00:00:23,620 --> 00:00:29,570
to be using pi and we'll be plotting out just a little bit just see what the numbers look like.

6
00:00:29,710 --> 00:00:35,760
So let's go ahead and import pipeline and we'll say map lib in line.

7
00:00:37,080 --> 00:00:37,370
OK.

8
00:00:37,380 --> 00:00:39,210
So let's go and grab our data.

9
00:00:39,600 --> 00:00:44,250
Remember senseful has a really nice function that allows you to quickly grab the data as you say from

10
00:00:44,250 --> 00:00:48,660
examples tutorials and just import input data

11
00:00:51,650 --> 00:00:57,680
and then we're going to say this is equal to input underscored data and we're going to say read data

12
00:00:57,680 --> 00:01:02,220
sets and I'm going to pass in the exact file path from the notebook.

13
00:01:02,370 --> 00:01:04,980
That way I don't accidently read download all the data.

14
00:01:05,130 --> 00:01:09,630
So I'm just going to copy and paste this to make sure I don't mess it up here.

15
00:01:09,630 --> 00:01:10,120
There we go.

16
00:01:10,170 --> 00:01:13,920
So I'm gonna grab it again from the convolutional neural networks and this dataset.

17
00:01:14,040 --> 00:01:19,500
When you run this you should only see extracting if you're running this in the same folder as the notebook

18
00:01:19,500 --> 00:01:19,980
expect.

19
00:01:19,980 --> 00:01:23,050
So keep that in mind though you don't need to download everything again.

20
00:01:23,100 --> 00:01:30,040
So a quick review remember if we want to show the data you can say this just training.

21
00:01:30,080 --> 00:01:36,080
And then there's images here and you can grab just a summary in the image here so whatever is the 12th

22
00:01:36,080 --> 00:01:42,510
index the image and then you need to reshape it to a 28 by 28 pixel run that and that should show you

23
00:01:42,510 --> 00:01:44,910
whatever image it is in this case it's a 1.

24
00:01:44,910 --> 00:01:49,510
So if we took out another image then this one looks like a 2 or 9.

25
00:01:50,100 --> 00:01:51,210
This one's a 7.

26
00:01:51,240 --> 00:01:52,080
Clearly.

27
00:01:52,080 --> 00:01:52,500
OK.

28
00:01:52,710 --> 00:01:59,670
And then remember that we can also say See map so it can say grace here just to see the actual white

29
00:01:59,670 --> 00:02:00,770
background.

30
00:02:01,290 --> 00:02:01,530
All right.

31
00:02:01,530 --> 00:02:06,240
Now it's time to actually build out the networks the first network we're going to start with is the

32
00:02:06,270 --> 00:02:11,190
generator network and then we're going to create the discriminated network for both these networks we're

33
00:02:11,190 --> 00:02:14,160
going to create what's called a variable scope.

34
00:02:14,160 --> 00:02:17,370
So let's first start with some functions.

35
00:02:17,370 --> 00:02:21,030
So we're going to create a generator function that we can easily call it.

36
00:02:21,140 --> 00:02:27,660
It takes Z as an input as he is going to be that random noise that we start the generator with and then

37
00:02:27,660 --> 00:02:34,580
it's going to have this redos variable that will explain later on when we talk about the variable scope.

38
00:02:34,590 --> 00:02:41,530
So we'll say with T.F. variable underscore scope we're going to pass in a string code here.

39
00:02:41,560 --> 00:02:46,660
If you shift tab you can see that there's a name or scope piercing and passenger string code will say

40
00:02:46,850 --> 00:02:49,570
GM for generator.

41
00:02:49,740 --> 00:02:52,650
And they're going to say reuse is equal to reuse.

42
00:02:52,650 --> 00:02:58,030
So you can either set this reuse as one or later on you're going to see is all set it as true.

43
00:02:58,050 --> 00:03:01,620
So you might be wondering well what is this variable scope for.

44
00:03:01,620 --> 00:03:10,320
Well the goal of variable scopes in general is to allow you to basically have modular sections or subsets

45
00:03:10,320 --> 00:03:16,290
of parameters so those belonging to certain layers that way when an architecture of a layer is repeated.

46
00:03:16,290 --> 00:03:21,090
So when you call this generator function again the same names of those variables can be used within

47
00:03:21,150 --> 00:03:22,620
each layer scope.

48
00:03:22,770 --> 00:03:26,370
That's going to make a little more sense when we actually see how we use this function because we're

49
00:03:26,370 --> 00:03:28,360
going to be using it over and over again.

50
00:03:28,410 --> 00:03:34,030
So keep that in mind that we're going to be using this variable's scope and this reuse.

51
00:03:34,040 --> 00:03:36,890
We're going to pass in later to be true.

52
00:03:36,950 --> 00:03:45,310
So what kind of touch on that later on right now will set the hidden layer for society of.

53
00:03:45,870 --> 00:03:48,890
And it's going to be a dense layer so densely connected.

54
00:03:49,170 --> 00:03:54,180
The inputs here is just going to be that random noise Z that will create later on and the number of

55
00:03:54,180 --> 00:03:57,020
units will just say 128 as we did before.

56
00:03:57,030 --> 00:04:03,660
When dealing with the state digits data set and then what we're going to do here is for our hidden layer

57
00:04:04,170 --> 00:04:11,190
the activation function we're going to be using is a leaky rectified linear unit So remember we talked

58
00:04:11,190 --> 00:04:16,020
about leaky rectified linear units we're talking about various activation functions.

59
00:04:16,050 --> 00:04:23,450
Unfortunately tensor flow in 1.3 still doesn't actually have a leaky rectified linear unit.

60
00:04:23,700 --> 00:04:27,330
And if you look this up on Stack Overflow let me bring in the link real quick.

61
00:04:27,330 --> 00:04:31,140
You'll get something like this where someone's asking well how can you use a leaky rectified linear

62
00:04:31,140 --> 00:04:31,740
unit.

63
00:04:31,860 --> 00:04:33,210
And there's a couple of good answers here.

64
00:04:33,210 --> 00:04:35,460
Let me zoom in so you can see this.

65
00:04:35,680 --> 00:04:39,680
If you take a look one of them actually just defines the full function here.

66
00:04:39,820 --> 00:04:46,240
And then another person has a really clever solution where you just use T.F. that maximum X or versus

67
00:04:46,270 --> 00:04:51,880
Alpha times X where you choose that Alpha basically essentially the leak rate of that leaky rectified

68
00:04:51,880 --> 00:04:52,640
linear unit.

69
00:04:52,840 --> 00:04:55,880
And this is a lot more efficient to use so we're going to be using that.

70
00:04:55,950 --> 00:05:01,780
And you'll notice at the very last post answered pretty recently is that there's going to be a leaky

71
00:05:01,810 --> 00:05:07,060
rectified linear unit upon the release of 1.4 which at the time of this filming isn't out yet and the

72
00:05:07,060 --> 00:05:09,940
environment we provide is not using 1.4.

73
00:05:09,940 --> 00:05:15,850
So eventually in the future you'll just be able to pasan TFA and that leaky rectified linear unit as

74
00:05:15,850 --> 00:05:17,170
the activation function here.

75
00:05:17,170 --> 00:05:24,220
Because remember for these layers if you do shift tab here there's going to be an activation option.

76
00:05:24,260 --> 00:05:28,300
OK so let's go ahead and do the maximum method.

77
00:05:28,330 --> 00:05:34,660
If we come back here we're going to be using this maximum Athen to simulate the activation function.

78
00:05:34,690 --> 00:05:36,840
So let's come over here and start typing that out.

79
00:05:38,020 --> 00:05:45,920
So we're going to choose an alpha of 0.01 for Aliki rectified linear unit and then we're going to do

80
00:05:45,920 --> 00:05:56,050
that kind of maximun logic here so I'll say T.F. maximum and we get to say Alpha times hit in one or

81
00:05:56,050 --> 00:05:59,200
hit and one says it's just going to choose the maximum out of that.

82
00:06:00,040 --> 00:06:05,280
And then we're going to do another hit and we'll say hit and Larry to again that's going to be T.F.

83
00:06:06,040 --> 00:06:13,360
layers that really connected the inputs for this one is going to be the up or hit in one and we'll have

84
00:06:13,360 --> 00:06:19,240
this be 120 units again and again something being play around of how many units you want.

85
00:06:19,240 --> 00:06:23,600
He wanted to you could have a really really large network if you wanted very good generation results

86
00:06:23,980 --> 00:06:25,800
but 128 should be more than fine.

87
00:06:25,810 --> 00:06:31,460
In fact you can kind of play around with if you only want one hidden layer but let's say hit insue now

88
00:06:31,980 --> 00:06:40,170
is going to be the same thing T.F. maximum and it will be Alpha times it into or hit into whichever

89
00:06:40,170 --> 00:06:42,060
one's maximum there.

90
00:06:42,160 --> 00:06:48,030
OK finally we're going to have our output and that's going to be T.F. layer's that's connected will

91
00:06:48,070 --> 00:06:50,030
take in it and layer 2.

92
00:06:50,430 --> 00:06:51,500
And then the units here.

93
00:06:51,510 --> 00:06:52,770
Because as the output.

94
00:06:52,790 --> 00:06:55,260
And we're actually generating a new picture.

95
00:06:55,290 --> 00:06:57,300
We're going to have the full 28 by 28.

96
00:06:57,540 --> 00:07:04,140
So we expect 784 to be our output here because we're actually trying to generate that vector image.

97
00:07:04,500 --> 00:07:10,650
And then the activation function when kind of playing around with this I notice that the hyperbolic

98
00:07:10,650 --> 00:07:13,710
tangent tended to work better than a sigmoid.

99
00:07:13,710 --> 00:07:16,790
So we'll go ahead and set up our network to use that.

100
00:07:16,800 --> 00:07:17,880
So say 10 H there.

101
00:07:17,910 --> 00:07:22,400
And later on you'll see that we're passing in random noise instead of going from zero to 1.

102
00:07:22,410 --> 00:07:24,040
We're going to go from negative 1 to 1.

103
00:07:24,060 --> 00:07:26,440
Just like the hyperbolic tangent does.

104
00:07:26,550 --> 00:07:30,240
And then finally we're going to return the output.

105
00:07:30,270 --> 00:07:33,000
OK so that is our generator function.

106
00:07:33,030 --> 00:07:37,770
We're going to do something really similar for our discriminator murmured the discriminator is going

107
00:07:37,770 --> 00:07:43,490
to try to tell whether something is real or fake from the generator.

108
00:07:43,530 --> 00:07:50,040
So let's go ahead and make this a discriminator and this is not going to take in Z.

109
00:07:50,050 --> 00:07:54,110
Instead it's going to take in data X but we'll have this for use.

110
00:07:54,150 --> 00:07:59,870
And instead of Jeon here we'll go ahead and say yes for the discriminator hit and one that's going to

111
00:07:59,870 --> 00:08:05,680
be the same thing except the inputs here is going to be X we'll go ahead and do the leaky rectified

112
00:08:05,680 --> 00:08:08,040
linear unit using this method of Alpha.

113
00:08:08,050 --> 00:08:09,970
So this is going to be the same.

114
00:08:09,970 --> 00:08:14,290
This is again going to be the same we'll have to hit in later here and at the very end the set of this

115
00:08:14,290 --> 00:08:17,880
activation function being a hyperbolic tangent we just want to know.

116
00:08:17,890 --> 00:08:19,030
True or false.

117
00:08:19,180 --> 00:08:22,420
Is it a real data point or not.

118
00:08:22,660 --> 00:08:28,840
So we're going to do is separate this out into two steps to say logic's themselves see a Flair's thence

119
00:08:29,450 --> 00:08:35,950
pass and head into and say units is equal to just 1 because we basically want the probability of it

120
00:08:35,950 --> 00:08:37,240
being real or fake.

121
00:08:37,410 --> 00:08:44,590
And then also the output is T.F. sigmoid of the logic's.

122
00:08:44,600 --> 00:08:46,670
OK so that's our scrimmage there.

123
00:08:46,670 --> 00:08:51,980
So now we have these two functions that we're going to be able to quickly call upon when we want to

124
00:08:51,980 --> 00:08:54,890
build out our generators and discriminators.

125
00:08:54,890 --> 00:08:55,310
OK.

126
00:08:55,520 --> 00:08:59,990
So in the very next lecture we're going to do is begin to actually use these functions along with some

127
00:08:59,990 --> 00:09:04,020
placeholders and then define a loss function as well as the optimizer.

128
00:09:04,040 --> 00:09:05,000
I'll see at the next lecture.

