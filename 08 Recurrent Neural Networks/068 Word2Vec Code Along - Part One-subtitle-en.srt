1
00:00:05,580 --> 00:00:10,950
Welcome everyone to the CO-2 long lecture for the words of ex-model we're going to be using the tensor

2
00:00:10,950 --> 00:00:15,480
for documentation example which has a tutorial implementation of war effect.

3
00:00:15,500 --> 00:00:17,990
It's a really great example for the documentation.

4
00:00:17,990 --> 00:00:19,750
So we'll just be using that directly.

5
00:00:19,790 --> 00:00:24,080
So that means we're going to be referring to the provided notebook often for blocks of code that we're

6
00:00:24,080 --> 00:00:26,720
going to end up copying and pasting to this code along.

7
00:00:26,720 --> 00:00:30,530
So make sure you have that notebook or the documentation example of ready to go.

8
00:00:31,750 --> 00:00:37,360
So as a quick note if we're to VAK is a model that really interests you further and you want to explore

9
00:00:37,360 --> 00:00:37,680
it.

10
00:00:37,870 --> 00:00:42,580
I would actually recommend that you don't use tensor flow as your main source of this but instead check

11
00:00:42,580 --> 00:00:44,010
out the gensym library.

12
00:00:44,020 --> 00:00:46,310
It's linked as a resource for this lecture.

13
00:00:46,310 --> 00:00:50,370
It has a much simpler to use API for word to Veck and additional functionality.

14
00:00:50,530 --> 00:00:55,140
I've used it myself in the past and it's a lot easier to use than tensor flow.

15
00:00:55,290 --> 00:00:59,920
Discourse is an senseful that will be showing you the tensor flow version but definitely check out the

16
00:00:59,920 --> 00:01:02,080
gensym library if you intend to use words.

17
00:01:02,260 --> 00:01:02,880
Further on.

18
00:01:02,880 --> 00:01:06,700
It has a lot more functionality and honestly it's a lot simpler to use.

19
00:01:06,700 --> 00:01:07,790
All right let's get started.

20
00:01:07,810 --> 00:01:08,960
In the Jupiter notebook.

21
00:01:09,040 --> 00:01:10,480
OK here I am of the Jupiter notebook.

22
00:01:10,480 --> 00:01:13,260
The first thing you need to do is our imports.

23
00:01:13,360 --> 00:01:15,970
Many imports that we actually have to do.

24
00:01:15,970 --> 00:01:18,540
So I'm just going to copy and paste them from the notebook.

25
00:01:18,580 --> 00:01:23,240
You can see you're using collection's math our operating system so we can grab the files zip files so

26
00:01:23,250 --> 00:01:25,420
we can unzip them as well as things like pie.

27
00:01:25,430 --> 00:01:26,890
And of course sensor flow.

28
00:01:26,920 --> 00:01:32,470
So we go ahead and run all those imports again just copy and paste that from the notebook and then up

29
00:01:32,470 --> 00:01:34,600
next will actually get the data.

30
00:01:34,600 --> 00:01:39,780
So there's a function that we're going to be creating that will be able to fetch the data for you from

31
00:01:39,780 --> 00:01:40,480
the your l.

32
00:01:40,540 --> 00:01:44,640
It's going to check if it exists or not and if it doesn't exist it will download it for you.

33
00:01:44,650 --> 00:01:47,350
So let's go and just create that.

34
00:01:47,390 --> 00:01:53,090
So we're going to say our data directory is equal to and match this exactly.

35
00:01:53,120 --> 00:01:55,310
Otherwise you'll end up downloading the data all over again.

36
00:01:55,310 --> 00:01:56,870
It's already downloaded for you.

37
00:01:57,610 --> 00:02:01,870
With the zip file so I'll say words effect underscore data words.

38
00:02:02,080 --> 00:02:06,760
And then in case you ever want to download the state of yourself you can use this function to do it.

39
00:02:06,940 --> 00:02:11,180
So I'm going to copy and paste this your L from the notebook just to make sure I don't mess it up.

40
00:02:12,110 --> 00:02:13,370
But it's basically right here.

41
00:02:13,550 --> 00:02:19,190
So we're going to use this text a zip file that comes from this Web site so you can download it directly

42
00:02:19,500 --> 00:02:24,230
in the function we're going to end up copying and pasting here is going to allow you to either download

43
00:02:24,230 --> 00:02:24,840
the data.

44
00:02:25,040 --> 00:02:29,600
If it doesn't exist or just open up the data in case you already have it download it.

45
00:02:29,600 --> 00:02:33,620
So if you come back here to recurrent networks you'll notice that we already have the words effect data

46
00:02:33,620 --> 00:02:34,280
for you.

47
00:02:34,400 --> 00:02:38,490
If you go in there words words zip zip file We're going to be using.

48
00:02:38,480 --> 00:02:44,870
So I'll go ahead and copy and paste the fecche words data function and then basically walk through what

49
00:02:44,870 --> 00:02:46,300
it's doing here.

50
00:02:46,310 --> 00:02:52,110
So when you call this fetch where it's data you end up passing in the URL and the words data directory.

51
00:02:52,310 --> 00:02:56,130
So what ends up doing it's going to make that directory if that does not exist.

52
00:02:56,210 --> 00:03:01,040
The circuitry already exist for us as we just saw under the current US networks folder then it's going

53
00:03:01,040 --> 00:03:03,050
to set a path to the zip file.

54
00:03:03,050 --> 00:03:07,010
So it's going to take that directory and append words that zip to it or join it.

55
00:03:07,010 --> 00:03:10,120
And then if the zip file isn't there it then downloads it.

56
00:03:10,310 --> 00:03:15,280
So this particular line won't work for you if you have some sort of restriction on your internet.

57
00:03:15,350 --> 00:03:19,670
So if you're on a work computer and you get some failure at this line it's probably because you have

58
00:03:19,670 --> 00:03:21,050
a firewall blocking you.

59
00:03:21,170 --> 00:03:25,080
So you'll need to get admin privileges privileges or turn off that firewall.

60
00:03:25,250 --> 00:03:30,020
So again keep that in mind you will need full admin privileges to download this zip file or you can

61
00:03:30,020 --> 00:03:32,990
just go to the Internet and download it manually from this you are.

62
00:03:33,290 --> 00:03:39,410
Then once the zip files there well we end up doing is we just read in that data and then we end up decoding

63
00:03:39,410 --> 00:03:40,220
it and splitting it.

64
00:03:40,220 --> 00:03:49,480
So this returns a list of all the words in the data source so say words is equal to and I mean to call

65
00:03:49,890 --> 00:03:51,790
fecche words data.

66
00:03:51,850 --> 00:03:53,170
Open and close parentheses.

67
00:03:53,200 --> 00:03:58,540
And I'm just using open unclose princes because I will be using the default values of data and data

68
00:03:58,540 --> 00:03:59,240
directory.

69
00:03:59,410 --> 00:04:04,670
So this should not download anything it's just going to fetch the data for me.

70
00:04:04,720 --> 00:04:06,360
Now that data is quite large.

71
00:04:06,400 --> 00:04:12,400
So this may take a little time for you but once we actually run that we'll see the length of the words

72
00:04:12,700 --> 00:04:14,560
is quite large.

73
00:04:14,560 --> 00:04:16,900
So we have about 17 million words here.

74
00:04:16,900 --> 00:04:18,280
So really big.

75
00:04:18,550 --> 00:04:25,000
Let's go out and get a slice of the words just so we can see them say something like get totally random

76
00:04:25,000 --> 00:04:31,420
slice here some 40 words so we can see feelings and the auditory system of a person with autism often

77
00:04:31,420 --> 00:04:36,280
cannot sense fluctuations etc. and you can see here this is what the words look like.

78
00:04:36,400 --> 00:04:42,370
So we're gonna end up doing is say print out kind of a sentence of these words.

79
00:04:42,370 --> 00:04:47,510
Now these sentences aren't exactly directly pulled from a text source.

80
00:04:47,800 --> 00:04:52,870
The zip file is specialized in a way to work a little better for words HVAC which is why it's kind of

81
00:04:52,870 --> 00:04:54,460
a sampling.

82
00:04:54,580 --> 00:04:58,000
So that's why we're using this link here.

83
00:04:58,060 --> 00:05:03,010
So even though these technically are sentences they've been adjusted a little bit so that they work

84
00:05:03,010 --> 00:05:04,580
a little better with words of that.

85
00:05:04,720 --> 00:05:08,330
So we'll say and is equal to just a space years.

86
00:05:08,330 --> 00:05:12,820
Print it all out in one line so we can see here what an example sentence may look like.

87
00:05:12,820 --> 00:05:17,610
So it says feelings and the auditory system of a person with autism often cannot sense the fluctuations

88
00:05:17,620 --> 00:05:22,600
what seems to not to sick people like a high pitched sing song or flat robot like voice is common in

89
00:05:22,600 --> 00:05:23,570
autistic children.

90
00:05:23,680 --> 00:05:25,650
Some autistic children and it continues on.

91
00:05:25,820 --> 00:05:28,120
You notice here that we've actually removed punctuation.

92
00:05:28,330 --> 00:05:31,520
So it kind of feels like a run on us because there are no periods here.

93
00:05:31,840 --> 00:05:36,130
And those are some of the adjustments for words effect so obviously doesn't make sense punctuation since

94
00:05:36,130 --> 00:05:38,770
we want kind of stream of words.

95
00:05:38,770 --> 00:05:40,660
So now we're going to build a word count.

96
00:05:40,870 --> 00:05:41,650
And it's really easy.

97
00:05:41,650 --> 00:05:47,410
The collections library we imported all say from collections import counter.

98
00:05:47,460 --> 00:05:50,030
And as you may have guessed counter we'll just count stuff.

99
00:05:50,040 --> 00:05:52,050
So I want to show you a quick example of what it looks like.

100
00:05:52,470 --> 00:06:07,000
If I make a list of words like one two and two and run that and then I say counter of my list it just

101
00:06:07,000 --> 00:06:09,640
returns back a dictionary with the actual word.

102
00:06:09,670 --> 00:06:11,110
And then how many times it showed up.

103
00:06:11,170 --> 00:06:14,010
So one showed up one time to shoot up two times.

104
00:06:14,110 --> 00:06:20,430
So you can also then request a method called most common and it would return back.

105
00:06:20,590 --> 00:06:26,020
However many most common words you asked so this returns back the number one most common word which

106
00:06:26,020 --> 00:06:27,190
in this case is two.

107
00:06:27,280 --> 00:06:31,690
And if you wanted the two most common words that you pass into and in this case that's all we have.

108
00:06:32,090 --> 00:06:35,860
OK so let's go ahead and create the word data and vocabulary.

109
00:06:36,010 --> 00:06:39,580
So we'll do another function here and this one we can actually code along with.

110
00:06:39,580 --> 00:06:41,940
Let me zoom in one more level here.

111
00:06:42,010 --> 00:06:50,600
We're going to say create the counts and we'll sorry General vocab size is going to be fifty thousand

112
00:06:50,600 --> 00:06:52,000
words.

113
00:06:52,050 --> 00:06:56,820
So even though we have many more words than that in our actual data set some of these are definitely

114
00:06:56,820 --> 00:07:01,230
repeat words so you can see here we have like persons going to be repeated the word was going to be

115
00:07:01,230 --> 00:07:02,900
repeated many times etc..

116
00:07:03,030 --> 00:07:10,490
So we'll have our model just take into account a total of 50000 unique words and we're going to say

117
00:07:10,490 --> 00:07:18,860
that vocab is equal to an empty list here plus counter of words.

118
00:07:19,460 --> 00:07:27,740
And then we're going to say the most common based off cup size.

119
00:07:28,160 --> 00:07:33,590
It's actually change this sort of being vocable will that vocab size sorry for this confusion they're

120
00:07:33,900 --> 00:07:37,080
sort of defining vocabulary size as 50000.

121
00:07:37,150 --> 00:07:43,570
So basically we are saying is Hey grab the most common 50000 words and that's going to be my vocabulary.

122
00:07:43,570 --> 00:07:45,760
So then I'm going to turn that into an umpire.

123
00:07:45,790 --> 00:07:47,910
Which is why I have that kind of list there.

124
00:07:47,910 --> 00:07:52,210
So we'll say vocab is equal to the array.

125
00:07:52,270 --> 00:07:56,190
Then I'll use miscomprehension to say word for word.

126
00:07:56,320 --> 00:08:02,920
Word for word comma underscores a little bit of typing a tuple in packing their vocab.

127
00:08:03,220 --> 00:08:05,660
So what I'm actually doing here with this tuple unpacking.

128
00:08:05,710 --> 00:08:11,170
Remember that what I asked for the most common I get back a list of tuples where the first item in the

129
00:08:11,170 --> 00:08:14,330
tuple is the word itself and the second item is the count.

130
00:08:14,390 --> 00:08:17,590
And I don't really care about the count I just care about the most common words.

131
00:08:17,620 --> 00:08:20,830
So what I'm doing here is using list comprehension for this vocab.

132
00:08:20,830 --> 00:08:26,980
All I'm saying is hey for the word in this tuple unpacking I'm only grabbing the word.

133
00:08:26,980 --> 00:08:31,460
And typically if you're going to throw something away in an underscore or not use it you just use.

134
00:08:31,540 --> 00:08:35,520
Scuse me when you throw something away in a list comprehension or a tuple on packing.

135
00:08:35,530 --> 00:08:37,120
You just use an underscore.

136
00:08:37,120 --> 00:08:40,960
So since I don't really care about this number I'm just going to put it in as an underscore.

137
00:08:40,960 --> 00:08:47,880
So that's common in Python if you're going to not use something just underscore their next.

138
00:08:47,920 --> 00:08:55,960
I'm going to create my dictionary and that's going to be word and I will say code for code.

139
00:08:57,320 --> 00:09:01,300
Word and enumerates vocab.

140
00:09:01,580 --> 00:09:04,490
And this is essentially a dictionary comprehension.

141
00:09:04,570 --> 00:09:08,660
So we'll see the result of that in just a little bit but then I will create my data.

142
00:09:09,260 --> 00:09:14,180
And this is going to be an umpire Ray and we're going to have a list comprehension here.

143
00:09:14,180 --> 00:09:25,420
So lots of comprehensions and Orsen essay gets word zero for word in words.

144
00:09:25,420 --> 00:09:30,040
And then finally we're going to return the data and the vocab.

145
00:09:30,090 --> 00:09:33,140
So let's run this and see what is actually being returned here.

146
00:09:33,150 --> 00:09:35,590
So again a vocab size is 50000.

147
00:09:35,610 --> 00:09:43,620
So I'll say the following data and my vocabulary are equal to create counts.

148
00:09:44,650 --> 00:09:47,360
So I'll run that again this may take a little bit of time.

149
00:09:47,560 --> 00:09:49,500
So I get to hop 4 in time until this is done.

150
00:09:50,680 --> 00:09:52,350
Or it actually just finished for me.

151
00:09:52,520 --> 00:09:54,830
So let's take a look at the shape of data.

152
00:09:55,890 --> 00:09:57,250
So the shape of data.

153
00:09:57,280 --> 00:09:59,180
Notice it's as many words as there were.

154
00:09:59,190 --> 00:10:03,490
And if we take a look at vocabulary and the shape it's 50000.

155
00:10:03,510 --> 00:10:04,890
So what does that actually mean.

156
00:10:05,110 --> 00:10:16,480
Well it means if I were to grab the value add words in that 100 then my data will tell me what number

157
00:10:16,490 --> 00:10:17,560
vocab word it is.

158
00:10:17,570 --> 00:10:26,120
So interpretations is 4189 out of my entire vocabulary so I can see here that the various words in my

159
00:10:26,120 --> 00:10:30,840
data have some sort of index position in my vocabulary.

160
00:10:30,850 --> 00:10:31,090
All right.

161
00:10:31,090 --> 00:10:36,720
So it's going to be useful later on when we generate batches OK the next step is to create a function

162
00:10:36,780 --> 00:10:39,200
that actually generates batches for us.

163
00:10:39,360 --> 00:10:45,180
Now we're going to copy this directly from the documentation so it's a bit complicated but basically

164
00:10:45,180 --> 00:10:49,410
what this is going to end up doing is at the end it's going to return a batch as well as the labels

165
00:10:49,470 --> 00:10:50,760
associated with it.

166
00:10:50,760 --> 00:10:54,630
So we provide a batch size and then number of skips and skip window which we're going to talk about

167
00:10:54,630 --> 00:10:55,290
later on.

168
00:10:55,440 --> 00:10:57,040
They're just going to be some parameters we can set.

169
00:10:57,070 --> 00:11:01,950
And then this whole code is going to generate the batches and you can check out the documentation if

170
00:11:01,950 --> 00:11:05,270
you want some more information about line by line what this is doing.

171
00:11:05,280 --> 00:11:08,580
But for our case we can just think of it as in the batches.

172
00:11:08,730 --> 00:11:13,470
And as I mentioned previously this is one of the reasons why you don't really want to use tensor flow

173
00:11:13,830 --> 00:11:17,910
if you're going to be using or HVAC because gensym kind of takes care of all this.

174
00:11:17,940 --> 00:11:19,760
For you the background.

175
00:11:20,260 --> 00:11:20,670
OK.

176
00:11:20,850 --> 00:11:23,470
So we have a function that's going to be able to generate the batches.

177
00:11:23,490 --> 00:11:29,300
Now we just need to create some constants and then we can start building the placeholders for our recession.

178
00:11:29,370 --> 00:11:33,250
So let's create some constants here will say constants.

179
00:11:33,270 --> 00:11:40,980
So we create a size of the actual batch schoolFeed in batches of let's say 128 at a time.

180
00:11:41,370 --> 00:11:45,670
And then we also need to say in imbedding size.

181
00:11:45,780 --> 00:11:51,060
So the imbedding size is basically going to be how many dimensions will the embedding vector have.

182
00:11:51,060 --> 00:11:56,090
The more they mention the more information but the more they mentions also the longer it takes a train.

183
00:11:56,100 --> 00:11:59,610
So a common size if you check out literature on this is 150.

184
00:11:59,610 --> 00:12:02,650
Again you can always expand this and you can always decrease it to fit.

185
00:12:02,670 --> 00:12:04,660
Training is taking too long.

186
00:12:04,770 --> 00:12:11,490
Then we're going to define a skip window and the skip window is going to be how many words to consider.

187
00:12:11,490 --> 00:12:12,990
To the left and to the right.

188
00:12:13,170 --> 00:12:15,470
So the bigger this value again the longer the training.

189
00:12:15,480 --> 00:12:20,850
So we'll keep it as small as possible just one and then we have the number of skips.

190
00:12:20,880 --> 00:12:25,530
So this is how many times to reuse an input to generate a label and we'll go ahead and keep that value

191
00:12:25,560 --> 00:12:27,330
low as well say 2.

192
00:12:27,930 --> 00:12:28,450
OK.

193
00:12:28,620 --> 00:12:32,400
So then we need to pick a random validation set to sample nearest neighbors.

194
00:12:32,400 --> 00:12:33,990
So what we actually mean by that.

195
00:12:34,050 --> 00:12:38,810
Well I'll say valid size is equal to 16.

196
00:12:38,820 --> 00:12:45,680
So this is a random set of words to evaluate similarity on and then we're only going to pick samples

197
00:12:45,830 --> 00:12:47,160
from the head of that distribution.

198
00:12:47,210 --> 00:12:59,100
So we'll say a valid window is equal to 100 and we'll say valid examples is equal to the random choice

199
00:13:00,420 --> 00:13:01,540
valid window.

200
00:13:02,680 --> 00:13:06,430
Valot size or say replace is equal to false.

201
00:13:09,210 --> 00:13:09,620
OK.

202
00:13:09,820 --> 00:13:12,730
So here what we're actually doing is we're limiting the validation samples.

203
00:13:12,730 --> 00:13:17,440
The words that have a low numeric ID which by the way we constructed these are also the most frequent

204
00:13:17,440 --> 00:13:18,350
words.

205
00:13:18,370 --> 00:13:26,890
So then let's go ahead and get a number of negative examples sample so see num sample is equal to 64

206
00:13:27,520 --> 00:13:30,700
and then we're going to choose our learning rate for our model.

207
00:13:31,150 --> 00:13:34,650
So a learning rate is equal to zero points or one.

208
00:13:34,810 --> 00:13:40,570
If you start using smaller learning rates it's really him to take a super long time and then we I believe

209
00:13:40,570 --> 00:13:42,240
we already defined our vocabulary size.

210
00:13:42,250 --> 00:13:43,740
But let's do it again just in case.

211
00:13:43,750 --> 00:13:44,300
We'll see.

212
00:13:44,370 --> 00:13:44,890
OK.

213
00:13:44,960 --> 00:13:52,150
Size is equal to 50000.

214
00:13:52,400 --> 00:13:57,030
OK so now let's go ahead and create our tensor flow placeholders and constants.

215
00:13:57,080 --> 00:14:03,050
So the first thing to do is reset my default graph because I mean using the same notebook here and then

216
00:14:03,050 --> 00:14:05,570
we want our input data.

217
00:14:05,670 --> 00:14:13,340
So we're going to create our training inputs to be T.F. placeholder and they're actually just integers

218
00:14:13,340 --> 00:14:21,420
this time and the shape here is just going to be none because the find by the Bache pups shape equals

219
00:14:24,470 --> 00:14:25,790
There we go next.

220
00:14:25,790 --> 00:14:31,410
It's going to be the training labels so we'll have that to T.F. da

221
00:14:34,080 --> 00:14:35,630
placeholder.

222
00:14:35,860 --> 00:14:44,320
That's also going to be a 32 bit integer and the shape here that's going to be the size by one.

223
00:14:44,540 --> 00:14:50,830
And then finally we have valid data set and we're going to synthetical to T.F..

224
00:14:50,830 --> 00:14:59,900
Constance valid examples for the data type is again just T.F. integer 32.

225
00:14:59,900 --> 00:15:04,640
All right so now we're going to create some variables and then we'll create our last function.

226
00:15:04,640 --> 00:15:09,260
So remember last function is the end loss.

227
00:15:09,360 --> 00:15:13,500
So let's create those variables will create our initial embeddings.

228
00:15:13,530 --> 00:15:17,890
So remember we need to have each of these words have some sort of random embedding first.

229
00:15:17,890 --> 00:15:24,000
So it's going to be T.F. random and we'll pick this up from a uniform distribution and we need to be

230
00:15:24,240 --> 00:15:26,230
by vocabulary size.

231
00:15:26,340 --> 00:15:32,900
So for each of the words and then by the embedding size so 150 It's make sure imbedding size.

232
00:15:33,400 --> 00:15:34,210
There we go.

233
00:15:34,440 --> 00:15:40,310
And then the other dimensions to be are those things are going to pass in are from negative 1 to 1.

234
00:15:40,480 --> 00:15:43,930
So we're going to randomly start off the vector embeddings.

235
00:15:43,990 --> 00:15:47,980
So we have 50000 words by a hundred fifty definitions for each word.

236
00:15:48,160 --> 00:15:52,250
And we just randomly choose values from negative 1 to 1.

237
00:15:52,320 --> 00:15:57,180
So that's going to initialize them and then we're going to say that the embeddings themselves are equal

238
00:15:57,180 --> 00:16:01,630
to T.F. variable.

239
00:16:02,090 --> 00:16:03,190
And it's in bits

240
00:16:06,990 --> 00:16:13,290
and then we're going to say embed is equal to T.F. and end and there's actually an embedding lookup

241
00:16:13,470 --> 00:16:15,450
function that's built into tensor flow.

242
00:16:15,660 --> 00:16:19,010
So this just looks up ideas in a list of imbedding tensors.

243
00:16:19,020 --> 00:16:20,590
Essentially what we created earlier.

244
00:16:20,730 --> 00:16:21,550
So we'll do the following.

245
00:16:21,550 --> 00:16:29,700
We'll say the embeddings we just made and then passing in the train that puts all right.

246
00:16:29,900 --> 00:16:34,210
So all we have left is to create the last function the optimizer and then run our session

247
00:16:37,840 --> 00:16:38,170
OK.

248
00:16:38,230 --> 00:16:42,910
All we have left is to create our last function our optimizer run the session and then visualize our

249
00:16:42,910 --> 00:16:43,630
results.

250
00:16:43,630 --> 00:16:45,850
So let's go ahead and do that in the very next lecture.

251
00:16:45,880 --> 00:16:46,550
I'll see you there.

