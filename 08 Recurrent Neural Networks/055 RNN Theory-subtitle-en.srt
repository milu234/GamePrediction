1
00:00:05,350 --> 00:00:10,000
Hello everyone and welcome to the recurrent neural networks theory lecture here in this lecture which

2
00:00:10,000 --> 00:00:15,490
is going to give you a brief overview of the theory behind a recurrent neural net.

3
00:00:15,490 --> 00:00:20,590
So first off we're going to mainly be working with recurrent neural networks to solve problems that

4
00:00:20,590 --> 00:00:24,570
deal with sequence information in a couple of examples of sequence information.

5
00:00:24,580 --> 00:00:27,370
Are things like generally time series data.

6
00:00:27,370 --> 00:00:32,560
So that could be something like a stock price changing throughout time or even things like the sales

7
00:00:32,560 --> 00:00:35,050
of a store changing throughout time.

8
00:00:35,050 --> 00:00:36,480
Then there's also sentences.

9
00:00:36,490 --> 00:00:39,890
So even just natural language has a sequence to it.

10
00:00:40,030 --> 00:00:43,130
So there's going to be words in a sentence in a particular order.

11
00:00:43,150 --> 00:00:45,390
So as you move along the words should be in a particular order.

12
00:00:45,400 --> 00:00:47,930
So that can also be thought of as a sequence.

13
00:00:47,980 --> 00:00:50,280
Then there's music and audio information.

14
00:00:50,410 --> 00:00:52,800
So that's sound and in particular at time ordering.

15
00:00:52,810 --> 00:00:58,150
So that can also be thought of a sequence and even things like contradictories so where a car is going

16
00:00:58,150 --> 00:01:03,850
to go or things like if you throw a ball up in the air or the trajectory of its path throughout the

17
00:01:03,850 --> 00:01:07,460
air it can also be thought of as a sequence.

18
00:01:07,620 --> 00:01:13,770
So we can imagine a sequence as just a vector of information for the index location basically points

19
00:01:13,770 --> 00:01:15,520
out its points in time.

20
00:01:15,690 --> 00:01:18,990
And then the actual values are the training values.

21
00:01:18,990 --> 00:01:24,960
So if we imagine a sequence that's 1 2 3 4 5 6 the question that you could ask anybody is are you able

22
00:01:24,960 --> 00:01:28,920
to predict a similar sequence shifted one tiny step into the future.

23
00:01:29,010 --> 00:01:30,750
And for most people it's actually pretty easy.

24
00:01:30,750 --> 00:01:35,700
You feel that there's a pattern here of increasing by 1 as you move along one time step.

25
00:01:35,700 --> 00:01:40,430
So if I asked you hey what's the sequence that's similar to this except shifted over one time that you

26
00:01:40,550 --> 00:01:41,670
would hopefully be able to figure out.

27
00:01:41,670 --> 00:01:44,840
Well I'll just say two three four five six seven.

28
00:01:44,940 --> 00:01:51,060
So we're going to try to later on training network to predict a time series shifted over one time step

29
00:01:51,120 --> 00:01:55,530
into the future and you can imagine that will be really beneficial if we're trying to predict sales

30
00:01:55,530 --> 00:01:56,440
for the next day.

31
00:01:59,300 --> 00:02:04,130
So the way we can do this is by using a recurrent neural network so let's review how the normal neuron

32
00:02:04,130 --> 00:02:06,290
works in a feed forward network.

33
00:02:06,350 --> 00:02:11,840
Remember that a normal neuron just takes in some input and it can be multiple inputs so it can aggregate

34
00:02:11,840 --> 00:02:17,150
them and then once it aggregates those inputs it passes it through some sort of activation function.

35
00:02:17,150 --> 00:02:21,950
Here we're just using the symbol for a rectified linear unit that it can be things like a sigmoid function

36
00:02:22,250 --> 00:02:24,110
or a 10:08 function et cetera.

37
00:02:24,290 --> 00:02:26,210
And then from that we have an output.

38
00:02:26,210 --> 00:02:30,810
So that's one normal neuron works a recurrent neuron is a little different.

39
00:02:30,860 --> 00:02:34,640
What it's going to do is send the output back to itself.

40
00:02:34,880 --> 00:02:39,590
So the output goes back into input of the same neuron.

41
00:02:39,950 --> 00:02:43,170
So we can actually unroll this throughout time.

42
00:02:43,460 --> 00:02:47,400
So if we roll this throughout time it ends up looking something like this.

43
00:02:47,510 --> 00:02:52,670
So we can see time here kind of represent it on the x axis and we have this particular recurrent neuron

44
00:02:53,090 --> 00:02:59,720
where it's input at T minus one gives an output at T-minus 1 and then that gets passed into the neuron

45
00:03:00,050 --> 00:03:04,520
in its state input at time t and then that has an output at time t.

46
00:03:04,520 --> 00:03:11,240
And then we can take that output and pass an input for the same neuron at time T plus 1 and then so

47
00:03:11,240 --> 00:03:12,140
on and so on.

48
00:03:12,140 --> 00:03:15,260
So this is called unrolling that recurrent neuron.

49
00:03:15,410 --> 00:03:20,180
Something that's important to note here is that the neuron is actually receiving both inputs from a

50
00:03:20,210 --> 00:03:24,390
previous timestep as well as inputs from the current time step.

51
00:03:24,400 --> 00:03:27,560
So you can see each of these neurons has two sets of inputs there.

52
00:03:27,710 --> 00:03:33,650
These cells that are a function of input from previous time steps are also known as memory cells in

53
00:03:33,650 --> 00:03:38,900
recurrent neural networks are also flexible in their inputs and outputs for both sequences and single

54
00:03:38,900 --> 00:03:42,390
vector values so show a couple of examples that in just a little bit.

55
00:03:42,560 --> 00:03:46,940
But I want to know it's also very easy to create a layer of recurrent neurons.

56
00:03:46,940 --> 00:03:51,980
Previously we just thought one recurrent neuron then we unrolled it through time but we could do the

57
00:03:51,980 --> 00:03:54,380
same thing for an entire layer.

58
00:03:54,470 --> 00:03:59,750
So if we want an entire layer of recurrent your aunts then it would look something like this where we

59
00:03:59,750 --> 00:04:03,890
have input X and then it goes through those recurrent neurons.

60
00:04:03,890 --> 00:04:05,180
Here we have three of them.

61
00:04:05,180 --> 00:04:10,940
Then we see the output y and then we take the output and then just pass it back in to all the neurons

62
00:04:10,970 --> 00:04:11,950
in that layer.

63
00:04:12,110 --> 00:04:17,960
And we could do the same idea and unroll this entire layer throughout time so that we get an input to

64
00:04:17,960 --> 00:04:26,180
equal zero and output it to equal zero and then pass that along with the output and input at time plus

65
00:04:26,180 --> 00:04:26,620
1.

66
00:04:26,730 --> 00:04:30,100
And same idea except now we're doing it with an entire layer.

67
00:04:30,140 --> 00:04:36,230
Now since the output of these recurrent neurons at a time that T is technically a function of all the

68
00:04:36,230 --> 00:04:42,540
inputs from previous times that you could then begin to think that has some form of memory because we're

69
00:04:42,560 --> 00:04:48,410
technically passing in historical information into that recurrent neuron or that layer of recurrent

70
00:04:48,410 --> 00:04:49,370
neurons.

71
00:04:49,370 --> 00:04:55,880
So part of this neural network that preserves some sort of stay across these types steps is called a

72
00:04:55,940 --> 00:05:01,120
memory cell and later on we're going to see much more complex examples of memory cells.

73
00:05:01,190 --> 00:05:07,250
But for now we have this basic recurrent neural near on that it just sends its output back into its

74
00:05:07,250 --> 00:05:07,930
input.

75
00:05:09,480 --> 00:05:13,420
Recurring year old networks are also very flexible in their inputs and outputs.

76
00:05:13,420 --> 00:05:18,640
So let's walk through a few examples of different combinations of inputs and outputs we can use for

77
00:05:18,640 --> 00:05:26,200
recurrent neural networks so we can actually perform a sequence input to a sequence output and an example

78
00:05:26,200 --> 00:05:32,780
of that would be passing in a set of Time series information such as a year's worth of daily sales data

79
00:05:33,180 --> 00:05:39,060
and then wanting back a sequence of that same sales data shifted over a certain time period into the

80
00:05:39,060 --> 00:05:39,600
future.

81
00:05:39,600 --> 00:05:44,010
And that's basically the initial example we first thought of that 1 2 3 4 5.

82
00:05:44,160 --> 00:05:51,420
And then you asked back for 2 3 4 5 6 TSOs a sequence however shifted over one step into the future

83
00:05:52,520 --> 00:05:53,730
another set of examples.

84
00:05:53,750 --> 00:05:59,710
Inputs and outputs would be to pass a sequence and put the request back a vector output.

85
00:05:59,780 --> 00:06:06,350
So a common example of using recurrent your own networks for this sort of input output is sentiment

86
00:06:06,350 --> 00:06:06,980
scores.

87
00:06:06,980 --> 00:06:13,550
So you can feed in a sequence of words maybe a paragraph of a movie review and then request back a vector

88
00:06:13,610 --> 00:06:17,490
indicating whether it was a positive sentiment such as they really liked the movie.

89
00:06:17,500 --> 00:06:22,460
So that's usually indicated by one versus they really dislike the movie which is indicated by usually

90
00:06:22,460 --> 00:06:24,020
negative 1 or 0.

91
00:06:24,020 --> 00:06:28,760
So that would be an example of a sequence input so that paragraph of words to just a single vector value

92
00:06:29,210 --> 00:06:33,770
and you essentially have a bunch of training data where you have a bunch of paragraphs and then some

93
00:06:33,770 --> 00:06:39,200
sort of sentiment score attached them and you train your recurrent neural network on that data to have

94
00:06:39,200 --> 00:06:41,690
a sequence input to a vector output.

95
00:06:41,750 --> 00:06:48,070
So on the opposite side you could also feed in a vector input so a single input at just a first time

96
00:06:48,140 --> 00:06:54,540
that and basically passen zeros for the rest of your time steps and then let the output be a sequence.

97
00:06:54,560 --> 00:06:58,180
So that's a vector to sequence network.

98
00:06:58,190 --> 00:07:05,150
So what you could do here is maybe pasan a single image and then request back a sequence of words describing

99
00:07:05,150 --> 00:07:06,740
the image that would be a caption.

100
00:07:06,920 --> 00:07:12,940
And if you've ever seen any research on line of images and auto generating captions you've seen examples

101
00:07:12,950 --> 00:07:18,080
of that where they show a picture of maybe a person on the beach and the neural network actually returns

102
00:07:18,080 --> 00:07:19,160
back a sentence.

103
00:07:19,160 --> 00:07:24,740
Man Standing on a beach or something of that nature obviously is a really complex networks take a lot

104
00:07:24,740 --> 00:07:29,150
of training time and a lot of data but it's definitely possible.

105
00:07:29,160 --> 00:07:33,530
So what we're going to do now is explore how we can build a simple recurrent in your own that work model

106
00:07:33,870 --> 00:07:39,420
with tensor flow manually then we're going to see how we can use tensor Flo's built in recurrent your

107
00:07:39,420 --> 00:07:41,170
own network API classes.

108
00:07:41,400 --> 00:07:46,110
So we're going to build a network manually still using tensor flow but a little more manually than usual.

109
00:07:46,110 --> 00:07:51,510
And then later on we'll see that Tanstaafl actually has a really nice API for recurrent neural networks

110
00:07:51,630 --> 00:07:54,540
that gives a nice level of abstraction to all of this.

111
00:07:54,540 --> 00:07:57,000
OK thanks everyone and I'll see you at the next lecture.

