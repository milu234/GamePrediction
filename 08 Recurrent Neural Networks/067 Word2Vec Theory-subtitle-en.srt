1
00:00:05,690 --> 00:00:10,460
Welcome everyone to this lecture where we're going to be discussing warts HVAC which is a really awesome

2
00:00:10,490 --> 00:00:17,720
algorithm or model that ends up embedding words into a vector of space.

3
00:00:17,910 --> 00:00:20,380
So we understand how to work with time series data.

4
00:00:20,550 --> 00:00:24,630
So we're going to take a look at another common series data source which is just words.

5
00:00:24,750 --> 00:00:27,740
So we can think of words in a sentence as a sequence.

6
00:00:27,750 --> 00:00:30,400
So for example a sentence can be thought of as a sequence like.

7
00:00:30,480 --> 00:00:35,480
Hi how are you now in classic natural language processing.

8
00:00:35,480 --> 00:00:41,090
Words are typically replaced by numbers and it indicates some sort of frequency relationship to their

9
00:00:41,090 --> 00:00:41,920
documents.

10
00:00:42,140 --> 00:00:47,860
And in doing this approach we end up losing information about the relationship between the words themselves

11
00:00:49,680 --> 00:00:54,510
so we can think of natural language processing as having two approaches either a counter based approach

12
00:00:54,570 --> 00:00:56,690
or a predictive based approach.

13
00:00:56,790 --> 00:01:03,870
So basically count based methods compute the statistics of how often some words co-occur with its neighbor

14
00:01:03,870 --> 00:01:09,810
words in a large teks corpus and then they end up mapping these counts autistics down to a small dense

15
00:01:09,810 --> 00:01:14,260
vector for each word essentially replacing the words with numbers.

16
00:01:14,280 --> 00:01:20,310
Now predictive models directly try to predict the word from its neighbors in terms of learned small

17
00:01:20,520 --> 00:01:22,200
dense embedding of vectors.

18
00:01:22,200 --> 00:01:24,990
And these are then considered the parameters of the model.

19
00:01:25,200 --> 00:01:29,610
So where its effect is going to take a predictive based approach where neighboring words are predicted

20
00:01:29,700 --> 00:01:31,170
based on the vector space.

21
00:01:31,350 --> 00:01:36,000
Instead of doing a classical count based approach where we essentially lose that sort of information

22
00:01:37,960 --> 00:01:39,270
so we can explore on your own that works.

23
00:01:39,270 --> 00:01:43,850
Most Famous use cases for this predictive based approach for natural language processing which is the

24
00:01:43,850 --> 00:01:48,150
word to VEC model created by Thomas Micallef.

25
00:01:48,350 --> 00:01:54,470
And again the goal of words of model is to learn the word embeddings by modeling each word as a vector

26
00:01:54,530 --> 00:01:59,750
in some dimensional space and later on we'll see that we actually have the capability to define how

27
00:01:59,750 --> 00:02:01,610
large of a dimensional space we want.

28
00:02:01,820 --> 00:02:07,520
So when we actually code this out we're going to find each word as a vector in a hundred and fifty them

29
00:02:07,520 --> 00:02:08,870
emotional space.

30
00:02:08,900 --> 00:02:14,540
So the question arises What is the actual motivation behind using word embeddings instead of just this

31
00:02:15,200 --> 00:02:17,460
approach that we discussed earlier.

32
00:02:17,480 --> 00:02:24,170
Well if we think about the representation of data across various data sources such as audio images or

33
00:02:24,170 --> 00:02:26,720
text we think about audio and images.

34
00:02:26,750 --> 00:02:32,110
These happened to be really rich high dimensional data sets that are also very dense in their information

35
00:02:32,120 --> 00:02:36,040
so you can have things like an audio spectrogram or image pixels.

36
00:02:36,110 --> 00:02:38,420
And these are really dense data sets.

37
00:02:38,420 --> 00:02:44,270
However when you take a approach like a count based approach to text data you end up getting a very

38
00:02:44,270 --> 00:02:46,070
sparse data set.

39
00:02:46,070 --> 00:02:52,340
So for tasks like object or speech recognition we actually already know that all the information required

40
00:02:52,640 --> 00:02:56,310
to successfully perform the task is encoded in the data.

41
00:02:56,510 --> 00:03:00,670
And we actually know this because humans themselves can perform tasks from the raw data.

42
00:03:00,710 --> 00:03:04,050
You can read a sentence and understand the meaning behind it.

43
00:03:04,100 --> 00:03:10,460
However natural language processing systems that traditionally treat words as discrete symbols will

44
00:03:10,520 --> 00:03:14,060
end up not being able to understand this information.

45
00:03:14,060 --> 00:03:20,270
So what I mean by that is a word such as cat is going to be represented as some number like the word

46
00:03:20,450 --> 00:03:27,890
5:37 and a word as such as dog is going to be represented as some other number like ID 6:59 or something

47
00:03:27,890 --> 00:03:28,990
of that nature.

48
00:03:29,000 --> 00:03:34,850
So you end up getting these arbitrary encodings that don't actually provide any useful information to

49
00:03:34,850 --> 00:03:40,530
the system regarding the relationships that may exist between those individual words or symbols.

50
00:03:40,550 --> 00:03:45,410
So you don't get to realize that dogs and cats are similar because they're both animals they're both

51
00:03:45,410 --> 00:03:47,930
popular as pets they both have four legs etc..

52
00:03:48,170 --> 00:03:52,440
You lose that information about the relationship between these words.

53
00:03:52,490 --> 00:03:59,210
Instead you just get information about the relationship of the frequency of these words showing up in

54
00:03:59,210 --> 00:04:07,060
a document so word to Veck trying to solve this problem by creating a vector based model that represents

55
00:04:07,120 --> 00:04:13,180
or in other words embeds the words into a continuous vector space with the words represented as vectors

56
00:04:13,240 --> 00:04:18,070
we can then actually do really cool things like perform vector mathematics on words which is kind of

57
00:04:18,070 --> 00:04:20,350
a crazy idea when you really think about it.

58
00:04:20,500 --> 00:04:25,810
So what that actually means is because each of these words is a vector you can do things like check

59
00:04:25,810 --> 00:04:31,030
how similar two vectors are to each other using something like cosigned similarity which means you can

60
00:04:31,150 --> 00:04:34,280
check how close and how similar two words are to each other.

61
00:04:34,440 --> 00:04:38,170
And then what's really cool is you can actually add and subtract vectors together.

62
00:04:38,350 --> 00:04:43,300
Meaning that once you actually embed these words as vectors you could technically add and subtract words

63
00:04:43,300 --> 00:04:44,880
to get different results.

64
00:04:45,710 --> 00:04:51,140
So at the start of training what happens is each embedding is random but through back propagation in

65
00:04:51,140 --> 00:04:56,230
the model we end up adjusting the value of each word vector in the given number of dimensions.

66
00:04:56,330 --> 00:04:59,830
So you slowly begin to adjust these vectors for these words.

67
00:05:00,080 --> 00:05:02,680
And more than mentions does mean more training time.

68
00:05:02,810 --> 00:05:05,380
But it also means more information per word.

69
00:05:05,390 --> 00:05:11,480
So what ends up happening is each dimension sometimes represents some sort of idea so similar words

70
00:05:11,480 --> 00:05:16,870
will end up being closer together as they're vectors are slowly brought closer together.

71
00:05:16,940 --> 00:05:21,530
And what's even more impressive as I mentioned the model is going to end up producing accessories that

72
00:05:21,620 --> 00:05:27,620
represent concepts or ideas so they may be mentioned that represents gender or another dimension for

73
00:05:27,620 --> 00:05:30,160
verbs singular vs plural etc..

74
00:05:31,700 --> 00:05:33,720
So what does this actually result in.

75
00:05:33,920 --> 00:05:37,750
Well you can end up plotting out the vectors and performing mathematics on them.

76
00:05:37,910 --> 00:05:44,150
So maybe you'll end up realizing that you can have two words such as man and King and realized that

77
00:05:44,150 --> 00:05:49,780
along a particular of them mention they have their gender counterparts such as woman and queen.

78
00:05:49,820 --> 00:05:56,570
So a lot of times there's a famous example where someone says a king Minus Man plus woman and the vector

79
00:05:56,570 --> 00:05:58,320
that's output is queen.

80
00:05:58,400 --> 00:06:02,870
So you can see that the model is actually learning the relationship and even the meaning behind some

81
00:06:02,870 --> 00:06:03,800
of these words.

82
00:06:04,040 --> 00:06:08,960
And then there's other dimensions such as understanding a verb tense so walking to walked or swimming

83
00:06:08,960 --> 00:06:14,500
to swam and then you end up getting a relationships between just even geographical location.

84
00:06:14,490 --> 00:06:21,230
So the model and the understanding that Madrid is related to Spain Rome is related to Italy etc. just

85
00:06:21,230 --> 00:06:27,170
based off the neighboring words which kind of makes sense because as you read some large corpus attacks

86
00:06:27,440 --> 00:06:29,730
you're more likely to find Berlin very close.

87
00:06:29,730 --> 00:06:34,530
The word Germany and you're more likely to find the word Tokyo very close to the word Japan.

88
00:06:34,540 --> 00:06:37,710
So those vectors end up having a relationship.

89
00:06:37,820 --> 00:06:44,390
Now it's actually discuss how words HVAC actually creates these word embeddings and how it learns these

90
00:06:44,390 --> 00:06:46,070
from raw text.

91
00:06:46,070 --> 00:06:52,310
So words of Veck actually comes in two flavors if you will and that is the continuous bag of words model

92
00:06:52,560 --> 00:06:58,760
and the skip Graham model and algorithmically speaking the models are really similar except in the way

93
00:06:58,760 --> 00:07:01,140
that they end up predicting target words.

94
00:07:01,370 --> 00:07:05,760
So let's actually first take a look at this bottom continuous bag of words approach.

95
00:07:05,810 --> 00:07:10,370
So the bag of words approach basically takes in a source context.

96
00:07:10,370 --> 00:07:17,090
Words such as the dog choose the and then attempts to find its prediction target which is just a single

97
00:07:17,090 --> 00:07:17,720
word.

98
00:07:17,720 --> 00:07:19,820
So that target word maybe something like bone.

99
00:07:19,820 --> 00:07:22,030
So you feed in the dog chews the.

100
00:07:22,100 --> 00:07:26,130
And it ends up trying to find the best target fit as just an individual word.

101
00:07:26,270 --> 00:07:31,670
And in that case it could be something like bone the skip gram model that one on the top does the exact

102
00:07:31,760 --> 00:07:35,970
inverse to it predicts source contex words from the target words.

103
00:07:35,990 --> 00:07:41,170
So you end up feeding it bone and it tries to predict the context around it such as the dog chews the

104
00:07:42,110 --> 00:07:43,220
SO for the most part.

105
00:07:43,340 --> 00:07:49,520
This turns out to be useful for larger data sets and the continuous bag of words approach tends to be

106
00:07:49,640 --> 00:07:56,060
better for smaller data sets and that's usually because statistically speaking the bag of words is going

107
00:07:56,060 --> 00:08:02,170
to smooth over a lot of the distributional information and it does that by treating an entire context

108
00:08:02,420 --> 00:08:04,160
as one observation.

109
00:08:04,160 --> 00:08:07,360
So again bag of words tends to be a little better for smaller data sets.

110
00:08:07,580 --> 00:08:14,720
OK so let's go ahead and basically talk about how it actually trains the model and it does this by something

111
00:08:14,720 --> 00:08:18,240
called Noise contrastive training.

112
00:08:18,260 --> 00:08:24,790
So the way this works if we think about the bag of words approach is that given the dog choose the blank

113
00:08:24,830 --> 00:08:29,200
we're looking for the word that's going to be the real target word.

114
00:08:29,360 --> 00:08:31,020
So we'll call that W T.

115
00:08:31,220 --> 00:08:33,460
And we know the real target word is bone.

116
00:08:33,560 --> 00:08:39,010
And then we also have a bunch of noise words throw our document such as book car house on guitar.

117
00:08:39,110 --> 00:08:45,580
So we're looking for the word that's most likely to fit there as the target so for me to visualize this

118
00:08:45,580 --> 00:08:47,500
it would look something like this.

119
00:08:47,500 --> 00:08:51,290
So again we're to Vic doesn't actually need a full probabilistic model.

120
00:08:51,460 --> 00:08:57,310
Instead what we end up doing is we use a binary classification objective such as a logistic regression

121
00:08:57,700 --> 00:08:59,710
to discriminate the real target words.

122
00:08:59,710 --> 00:09:05,390
That is WFC from imaginary noise words we'll call those K in the same context.

123
00:09:05,410 --> 00:09:10,660
So this is basically a simple visualization where we have our projection layer.

124
00:09:10,780 --> 00:09:12,640
The cat sits on the blank.

125
00:09:12,640 --> 00:09:15,200
So Matt in this case so that's the target word.

126
00:09:15,340 --> 00:09:17,610
And we're going to have some sort of noise classifier.

127
00:09:17,640 --> 00:09:23,110
So it's a binary classification where we compare that target word versus the rest of these noise words

128
00:09:23,110 --> 00:09:28,070
and then we pass that in through some hidden layers and eventually we get some sort of embeddings.

129
00:09:28,070 --> 00:09:31,620
So our training process is going to be noise contrastive training.

130
00:09:31,660 --> 00:09:37,580
We're basically contrasting the noise versus the actual target word.

131
00:09:37,630 --> 00:09:40,970
So the target word is predicted by maximizing this equation right here.

132
00:09:41,140 --> 00:09:44,000
And don't worry too much about memorizing every little detail here.

133
00:09:44,020 --> 00:09:48,720
Instead try to understand the basic idea so let's look at that first term of the equation.

134
00:09:48,820 --> 00:09:54,340
So that first term is the binary logistic regression probability under the model of seeing the word

135
00:09:54,430 --> 00:10:02,850
w or W of T in the context H for some dataset capital D and it's cockily in terms of the learn them

136
00:10:02,860 --> 00:10:05,540
betting vectors theta.

137
00:10:05,540 --> 00:10:11,950
So in practice we approximate the expectation by drawing a contrast of words from the noise distribution.

138
00:10:12,140 --> 00:10:14,480
So here we're only drawing K words.

139
00:10:14,480 --> 00:10:17,780
We're not drawing the entire corpus text data.

140
00:10:17,780 --> 00:10:21,660
Instead we're just drawing a certain amount of words for the noise distribution.

141
00:10:21,680 --> 00:10:27,590
So this works really well for us because computationally it's really efficient because now we're just

142
00:10:27,590 --> 00:10:29,030
computing the last function.

143
00:10:29,180 --> 00:10:32,270
And it scales only to the number of noise words that we select.

144
00:10:32,270 --> 00:10:37,610
That is that case number not all the words in the vocabulary which would be let's say capital V or something

145
00:10:37,610 --> 00:10:38,510
of that nature.

146
00:10:38,540 --> 00:10:44,060
So that makes it a lot faster to train because we're only drawing keywords as are noise words instead

147
00:10:44,060 --> 00:10:46,690
of drawing and comparing to the entire vocabulary.

148
00:10:46,850 --> 00:10:51,740
So that's what makes effect computationally efficient for the task it's actually doing.

149
00:10:51,740 --> 00:10:56,240
Now again the larger the amount of keywords you end up drawing the longer your training time is going

150
00:10:56,240 --> 00:10:56,830
to take.

151
00:10:56,870 --> 00:11:02,570
But that should then make it more accurate to predict the target word versus a wide variety of noise

152
00:11:02,570 --> 00:11:03,560
words.

153
00:11:03,860 --> 00:11:09,380
However you'll see you can get really reasonable results for smaller values of k.

154
00:11:09,400 --> 00:11:15,170
So again the goal is to assign high probability to correct words and low probability to noise words.

155
00:11:15,250 --> 00:11:19,260
Then once you've ran through this model process you'll have vectors for each word.

156
00:11:19,420 --> 00:11:24,400
And what's really cool is you can visualize the relationships through some sort of dimension reduction

157
00:11:24,730 --> 00:11:30,280
and a really popular one especially for Wurtz HVAC is t distributed the classic neighbor imbedding.

158
00:11:30,310 --> 00:11:35,650
So in our practice we're going to have each vector be represented by a hundred and fifty dimensions

159
00:11:35,920 --> 00:11:38,510
and then we're going to use this process to reduce it down to two.

160
00:11:38,620 --> 00:11:43,390
So then we just have these words represented by two missional vectors which means we can then plot them

161
00:11:43,390 --> 00:11:46,670
out because we have two coordinates per word.

162
00:11:46,840 --> 00:11:49,600
So that ends up giving you something that looks like this.

163
00:11:49,600 --> 00:11:54,370
So this is actually the final plot we'll be creating and hopefully we'll end up realizing that points

164
00:11:54,370 --> 00:11:57,040
that are close to each other tend to have some similarity.

165
00:11:57,880 --> 00:12:00,050
All right let's get started in the next lecture.

166
00:12:00,070 --> 00:12:01,830
We'll code out the words havock model.

