1
00:00:05,250 --> 00:00:09,160
Welcome everyone to part two of the solutions for the Safar it's an exercise.

2
00:00:09,360 --> 00:00:12,660
Let's go right to where we left off last time and finish this up.

3
00:00:12,660 --> 00:00:14,870
All right so mainly the solutions.

4
00:00:14,940 --> 00:00:19,980
One was just going through the code that was already provided for you in understanding how it actually

5
00:00:19,980 --> 00:00:23,010
works and how it actually produces images.

6
00:00:23,010 --> 00:00:24,780
Now it's time to actually create the model.

7
00:00:24,930 --> 00:00:33,380
So we're going to do the following I'll say imports tensor flow as T.F. and then we're going to create

8
00:00:33,380 --> 00:00:38,010
two placeholders one for data X and then y true.

9
00:00:38,060 --> 00:00:39,770
So their shapes should make sense to you.

10
00:00:39,770 --> 00:00:49,620
Now we have T-Rexes a placeholder and it's going to be T.F. flow thirty two or three Remember the need

11
00:00:49,620 --> 00:00:56,010
to provide a data type and the shape is just going to be none because that's dictated by the batch size.

12
00:00:56,150 --> 00:01:06,360
And then we're doing 3 2 3 2 3 3 2 3 2 pixels three color channels then we're going to say why true

13
00:01:07,140 --> 00:01:13,380
is equal to a placeholder as well to 32 as well.

14
00:01:13,380 --> 00:01:17,390
And then this shape is just going to be none because of the batch size.

15
00:01:17,400 --> 00:01:20,130
And then 10 because it's one encoded.

16
00:01:20,210 --> 00:01:26,690
So again your typical shapes is going to be images height width and then channels just like we did for

17
00:01:26,690 --> 00:01:29,320
the digits data set.

18
00:01:29,350 --> 00:01:34,900
So I had these two placeholders for my data and my labels and I'm going to create one more cold hold

19
00:01:34,900 --> 00:01:42,370
probability and no need for shape here because it's essentially going to be a single number of what

20
00:01:42,370 --> 00:01:50,990
percentage you want for the probability that neurons are being hold when we actually perform dropout.

21
00:01:50,990 --> 00:01:55,110
All right so now we have a whole probability then we have the helper functions.

22
00:01:55,130 --> 00:01:59,600
So you have the option of grabbing the helper functions or seeing if you can recreate them yourself

23
00:01:59,600 --> 00:02:00,780
for hard challenge.

24
00:02:00,950 --> 00:02:07,850
Well just go ahead since you've already gone through these in great detail during the mist with CNN

25
00:02:08,810 --> 00:02:10,000
we'll just copy and paste them here.

26
00:02:10,010 --> 00:02:12,760
So remember we initialize weights initialized bias.

27
00:02:12,860 --> 00:02:17,850
We have these helper functions for a to the convolution Max pool two by two.

28
00:02:18,050 --> 00:02:19,100
And then convolutional.

29
00:02:19,130 --> 00:02:20,660
And normal full layer.

30
00:02:20,660 --> 00:02:25,940
And if you want you can kind of play around with these sizes maybe you want to see three by three pulling

31
00:02:25,940 --> 00:02:29,510
or maybe you want to do different strides for convolutional to the.

32
00:02:29,540 --> 00:02:30,650
Again totally up to you.

33
00:02:30,860 --> 00:02:33,980
But we'll go ahead just use the same values as last time.

34
00:02:33,980 --> 00:02:35,640
Should be fine for our purposes.

35
00:02:35,810 --> 00:02:38,570
And now it's time to create the letters.

36
00:02:38,690 --> 00:02:43,000
So we're essentially going to create the same type of layers as we did last time.

37
00:02:43,040 --> 00:02:48,500
Again a lot of these sizes are up to you just some things are going to be aware of the fact that three

38
00:02:48,500 --> 00:02:52,430
color channels and that the shapes are 32 by 32.

39
00:02:52,490 --> 00:02:54,870
So we'll start off creating convo 1.

40
00:02:54,930 --> 00:03:02,550
So our first convolution layer is convo underskirt one that's going to be a convolutional Lehre takes

41
00:03:02,560 --> 00:03:09,310
center data X and then our shape will go ahead and have it be four by four.

42
00:03:10,510 --> 00:03:12,730
And then we'll say three and three to

43
00:03:15,650 --> 00:03:21,880
and then we're going to do a pooling layer after this all say convoke one pooling.

44
00:03:22,600 --> 00:03:25,780
And that's just going to be equal to that Max pool function.

45
00:03:25,840 --> 00:03:34,150
And then we passen Canova 1 now it's up to you what size you want these actual convolutional filters

46
00:03:34,150 --> 00:03:39,560
to be here we're just using four by four but you can always play around with those values.

47
00:03:39,610 --> 00:03:43,280
So next we want to create in other convolutional there.

48
00:03:43,300 --> 00:03:50,710
So it's going to do that we'll say Canova 2 is equal to a convolutional layer and then we're going to

49
00:03:50,720 --> 00:04:01,590
input the result of the first pooling will say shape is equal to 4 4 and then all going to say 32 64

50
00:04:01,590 --> 00:04:13,300
here and then we're going past that into pulling so convoke to pooling is equal to max pool two by two

51
00:04:13,360 --> 00:04:17,950
and then pass and the result of convo to OK.

52
00:04:17,980 --> 00:04:21,400
So we have to convolutional errors leading into two pulling layers.

53
00:04:21,440 --> 00:04:26,780
So we have kind of one leading and one pulling that leads into the second convolutional there and then

54
00:04:26,780 --> 00:04:28,130
we have pulling after that.

55
00:04:28,460 --> 00:04:34,670
So next we're going to create a flattened layer by reshaping that pulling layer into minus 1 8 by 8

56
00:04:34,670 --> 00:04:41,000
time 64 or essentially negative 1 four thousand ninety six and we did a really similar step with the

57
00:04:41,820 --> 00:04:43,370
digits dataset.

58
00:04:43,430 --> 00:04:53,100
So we're going to see convo to flat and always say tenderfoot reshape going to reshape that last puling

59
00:04:53,490 --> 00:05:04,220
layer and we're going to reshape it to negative one by eight times eight times 64 so that you create

60
00:05:04,220 --> 00:05:12,460
a new fooler using the normal full air function using this fine layer all these 1024 neurons will say

61
00:05:12,470 --> 00:05:16,450
full layer 1 is equal to T.F..

62
00:05:16,460 --> 00:05:21,800
And again this is going to be a normal error that uses the rectified linear unit.

63
00:05:22,920 --> 00:05:31,900
As the activation functions will say normal full convo to flat 1024.

64
00:05:31,910 --> 00:05:33,770
Next we're going to create the dropout layer.

65
00:05:33,800 --> 00:05:37,060
So that's also pretty straightforward we'll say.

66
00:05:38,190 --> 00:05:43,560
Fool ONE drop out is equal to tensor flow.

67
00:05:43,560 --> 00:05:51,980
Neural Network dropout full air 1 and we'll say the probability for being held is equal to that placeholder

68
00:05:51,980 --> 00:05:56,730
recreated hold prob and hopefully this feels really familiar.

69
00:05:57,020 --> 00:06:03,030
This needs to be keep prob not just prob keep prob there we go.

70
00:06:03,230 --> 00:06:08,240
And like I was saying hopefully it feels really familiar because this is almost in line directly from

71
00:06:08,240 --> 00:06:10,540
the examples that we walk through.

72
00:06:10,970 --> 00:06:17,810
So finally we need to set the output so we'll say that our predicted y value is equal to a normal flare

73
00:06:18,410 --> 00:06:25,750
that has the full one dropout layer and then 10 year on is here because there's 10 possible errors.

74
00:06:25,760 --> 00:06:28,970
Now we're going to use a loss functionally as a cross entropy loss function.

75
00:06:30,200 --> 00:06:36,800
Well say cross entropy is equal to T.F. reduce mean.

76
00:06:37,000 --> 00:06:44,140
And then we'll say T.F. and thought and then we call soft Max cross entropy and will provide the labels

77
00:06:44,140 --> 00:06:46,950
to be quite true.

78
00:06:48,650 --> 00:06:51,490
And the logic is to be why spreads are predicted values.

79
00:06:51,490 --> 00:06:57,560
They're essentially comparing our predictions to the true values that we want an optimizer.

80
00:06:57,860 --> 00:06:59,710
So we'll say optimizer.

81
00:07:00,020 --> 00:07:06,250
And this is basically the same things we did last time is T.F. train we're going to use an atom optimizer

82
00:07:08,680 --> 00:07:14,590
and we'll set the learning rate to be zero point zero zero or 1 something you can kind of play around

83
00:07:14,590 --> 00:07:16,000
with there as well.

84
00:07:16,340 --> 00:07:20,100
And then we want to use that optimizer to minimize our loss.

85
00:07:20,260 --> 00:07:25,960
So say optimizer minimize cross entropy.

86
00:07:26,950 --> 00:07:32,500
Next create that in a variable that we always do to initialize everything so global variables initialiser

87
00:07:33,780 --> 00:07:34,190
OK.

88
00:07:34,230 --> 00:07:35,660
Now it's time for the graph session.

89
00:07:35,680 --> 00:07:38,940
So you can see here we already have some output so I'm going to clear that by running in that cell that's

90
00:07:39,000 --> 00:07:39,570
empty.

91
00:07:39,930 --> 00:07:47,120
And let's build that will say with T.F. session as SPSS.

92
00:07:48,010 --> 00:07:50,910
I'm going to say a session run will say T.F..

93
00:07:51,030 --> 00:07:52,590
Actually you just need to say in it's here

94
00:07:55,670 --> 00:07:58,430
and then for I in range.

95
00:07:58,790 --> 00:08:01,620
And this is up to you how many train steps you want to do.

96
00:08:01,950 --> 00:08:02,910
I did five thousand.

97
00:08:02,910 --> 00:08:04,360
So let's try that out.

98
00:08:05,590 --> 00:08:12,370
Our batch is going to be equal to C.H. the next batch.

99
00:08:12,400 --> 00:08:16,690
So we want to make sure that we provide a batch size in there.

100
00:08:16,690 --> 00:08:23,460
So if we scroll back up remember there's instructions here and how to actually use the above code to

101
00:08:23,500 --> 00:08:25,100
make sure that we actually ran it.

102
00:08:25,120 --> 00:08:29,120
So we're going to create an instance of that Safar helper and then set up the images.

103
00:08:29,470 --> 00:08:35,230
So if you run this lips Safar help result define wups for author on these cells.

104
00:08:35,230 --> 00:08:40,950
There we go and let's make sure 100 codes not define that or come up here and run this one too.

105
00:08:40,960 --> 00:08:43,890
So now that I ran these two there you go.

106
00:08:44,110 --> 00:08:46,900
So it set up the training images labels set up test images labels.

107
00:08:46,900 --> 00:08:47,840
It's ready to go.

108
00:08:47,950 --> 00:08:52,970
So should be ready to feed batches using this next batch command will come back down here.

109
00:08:54,330 --> 00:08:57,790
It will say we'll grab a at a time.

110
00:08:58,000 --> 00:09:02,150
And as I mentioned earlier you can change about if you want.

111
00:09:02,170 --> 00:09:10,420
There was a session run train and we need to provide a feed dictionary will say x is equal to match

112
00:09:10,600 --> 00:09:12,900
zero.

113
00:09:13,260 --> 00:09:24,240
Why true is then going to be that one and we'll provide a hold probability of 0.5 just like we did last

114
00:09:24,240 --> 00:09:24,660
time.

115
00:09:24,660 --> 00:09:29,850
So a 50 percent chance of being held next we're going to run some tests here.

116
00:09:29,870 --> 00:09:38,090
So I will say if I mod 100 as equal to zero meaning or on a hunter's step are basically a step that's

117
00:09:38,090 --> 00:09:39,620
the visible by a hundred.

118
00:09:39,770 --> 00:09:45,370
Say Prince the step and we'll just say that format.

119
00:09:45,380 --> 00:09:48,410
I say things with it and this dataset.

120
00:09:48,450 --> 00:09:50,640
Let's zoom in here to see it clearly.

121
00:09:52,050 --> 00:10:01,100
Then we're going to do the exact same thing as the last time we'll say matches T.F. equal T.F. RMX of

122
00:10:01,170 --> 00:10:10,980
widespread one and they'll say T.F. RMX of y true come a 1.

123
00:10:11,010 --> 00:10:14,390
So that returns that Boulia a list of where they matching.

124
00:10:14,430 --> 00:10:21,460
Set up an accuracy measurement which is going to be reduced mean and I want to cast that list of booleans

125
00:10:22,880 --> 00:10:33,390
So when the cast matches to be to float 32 and then I'm going to print out session run run that accuracy

126
00:10:34,180 --> 00:10:37,430
and then we want to provide a feed dictionary now of the test data sets.

127
00:10:39,320 --> 00:10:46,850
So we do that is we basically just call it off that Helper Object that we created so we can say S.H.

128
00:10:47,670 --> 00:10:53,270
grabbed the test images.

129
00:10:53,460 --> 00:10:58,230
Why true it's going to be S.H. groud the test labels.

130
00:10:58,230 --> 00:11:07,140
So these are attributes from the subject and they will say hold probability is just 1.0.

131
00:11:07,300 --> 00:11:11,900
So we want everything to be held while we're testing and old print new line

132
00:11:14,860 --> 00:11:15,290
OK.

133
00:11:15,350 --> 00:11:18,590
Let's go in and run this check for any typos I'm sure I made one or two.

134
00:11:19,040 --> 00:11:20,200
So run that.

135
00:11:20,720 --> 00:11:20,960
OK.

136
00:11:20,960 --> 00:11:22,840
Looks like we actually are doing pretty good.

137
00:11:23,090 --> 00:11:30,020
So immediately right off the bat we're jumping straight to 50s and I'm going to fast forward in time

138
00:11:30,110 --> 00:11:32,630
until this is done training for 5000 steps.

139
00:11:32,720 --> 00:11:36,740
But you can already see we're kind of approaching 70 percent accuracy which is kind of what we're shooting

140
00:11:36,740 --> 00:11:37,490
for.

141
00:11:37,490 --> 00:11:41,660
So let me jump forward in time until this is done training for 5000 steps.

142
00:11:44,180 --> 00:11:44,540
OK.

143
00:11:44,560 --> 00:11:49,780
So after around 5000 steps looks like we just barely hit that 70 percent accuracy mark that we were

144
00:11:49,780 --> 00:11:51,930
looking for.

145
00:11:52,150 --> 00:11:56,080
But mind this may take a long time to run on your computer the pending on how fast the computer you

146
00:11:56,080 --> 00:11:58,310
have especially if you're just using CPQ.

147
00:11:58,570 --> 00:12:04,040
Right now I'm running this with GPS so it only took me around one or two minutes to run this whole thing.

148
00:12:04,060 --> 00:12:09,280
It could take definitely much much longer depending on how powerful your computer is and if your computer

149
00:12:09,670 --> 00:12:12,100
is really low power you may even run out of memory.

150
00:12:12,100 --> 00:12:13,680
So kind of keep that in mind.

151
00:12:13,720 --> 00:12:19,240
You can always use other services like AWOS or paper space to try to run some of this online.

152
00:12:19,240 --> 00:12:19,950
OK.

153
00:12:20,020 --> 00:12:22,300
If you have any questions feel free to post the Q&amp;A forums.

154
00:12:22,300 --> 00:12:24,420
I hope you found this exercise enjoyable.

155
00:12:24,420 --> 00:12:27,150
And now we definitely encourage you to kind of play around of some of these values.

156
00:12:27,160 --> 00:12:32,140
Now you play around the shapes of the filters and just really make sure you understand the basic flow

157
00:12:32,140 --> 00:12:32,720
here.

158
00:12:32,740 --> 00:12:37,240
It should feel really analogous to what we did together with the data.

159
00:12:37,300 --> 00:12:41,860
Again really hard part for when you're dealing with your own data is basically creating this class that

160
00:12:42,100 --> 00:12:44,290
kind of handles the images and reshapes them.

161
00:12:44,290 --> 00:12:48,820
I would say is probably by far the hardest part because as far as the convolutions are concerned a lot

162
00:12:48,820 --> 00:12:54,910
of that there are just too lenience calls and just taking advantage of tensor Fullers built in functionality.

163
00:12:54,910 --> 00:12:57,970
All right thanks everyone and I'll see you at the next section of the course.

