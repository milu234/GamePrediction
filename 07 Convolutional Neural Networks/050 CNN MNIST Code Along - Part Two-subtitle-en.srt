1
00:00:06,870 --> 00:00:08,050
Welcome back everyone.

2
00:00:08,070 --> 00:00:12,840
Here we are right where we left off last time we've defined all our layers and all that's left to do

3
00:00:12,930 --> 00:00:17,910
is create a lost function our optimizer initialize the variables and then run a session.

4
00:00:17,910 --> 00:00:25,830
So let's start for a last function we'll use cross entropy to be our last function and we essentially

5
00:00:25,830 --> 00:00:32,770
just use the built in functions that are already intenser for us so we say reduce mean to take an average

6
00:00:33,520 --> 00:00:41,180
and we'll say soft Max cross entropy and we're going to say that the labels is equal to y true which

7
00:00:41,190 --> 00:00:50,770
will be provided by our dictionary later on and then we'll say logic is to why spreads why.

8
00:00:50,810 --> 00:00:59,290
There we go OK it's all run that and then what we're going to do is set up our optimizer So our optimizer

9
00:00:59,530 --> 00:01:05,440
we're going to go ahead and use an atom optimizer say Adam optimizer and then we'll say the learning

10
00:01:05,440 --> 00:01:14,290
rates is zero point zero 0 1 and then our trainer is just going to beat for this optimizer to minimize

11
00:01:14,290 --> 00:01:20,410
our loss which in our case is the cross entropy loss.

12
00:01:20,590 --> 00:01:27,990
Then we're going to initialize our variables so say an equal to T.F. global variables initializer OK

13
00:01:28,000 --> 00:01:30,330
now it's time to define our session.

14
00:01:30,610 --> 00:01:33,910
So I'll say we'll run this for 5000 steps.

15
00:01:33,910 --> 00:01:39,190
Keep in mind I'm running on a really fast computer it's also a GPU accelerated on a very good high end

16
00:01:39,190 --> 00:01:42,270
graphics card so you may have to wait a while.

17
00:01:42,280 --> 00:01:45,090
Much longer than I am to get results.

18
00:01:45,100 --> 00:01:47,480
This could take up to several hours so keep that in mind.

19
00:01:47,590 --> 00:01:49,480
Depending on the speed of your computer.

20
00:01:49,970 --> 00:01:59,890
So say session run in it and then we'll say for I in range steps and again reduce the number of steps.

21
00:01:59,970 --> 00:02:05,460
If you have a slower computer you won't end up getting as accurate results but they'll still run and

22
00:02:05,460 --> 00:02:09,690
will say batch X that y.

23
00:02:09,750 --> 00:02:16,140
Same deal as last time we're basically just grabbing from the next batch 50 so we'll do 50 images at

24
00:02:16,140 --> 00:02:23,970
a time and this batch and then we're going to say session run run the training the optimizer and then

25
00:02:23,970 --> 00:02:25,640
we need feed dictionary.

26
00:02:25,970 --> 00:02:28,410
So remember that we actually have three placeholders.

27
00:02:28,620 --> 00:02:31,360
We have the two classic ones which is just x and y.

28
00:02:31,400 --> 00:02:37,230
So we'll say X is batch x and y true is that y.

29
00:02:37,530 --> 00:02:43,100
And then we have the hold probability because remember we are designing that dropout layer appear.

30
00:02:43,260 --> 00:02:48,630
We have this whole probability which is a placeholder and that basically is the probability that a neuron

31
00:02:48,630 --> 00:02:50,370
is held during dropout.

32
00:02:50,610 --> 00:02:53,870
So during training we'll go ahead and say 50 percent.

33
00:02:53,940 --> 00:02:59,980
So each near on has a 50 percent chance of being held essentially randomly dropping out half our neurons

34
00:03:01,330 --> 00:03:06,600
they'll say if I bought one hundred equals equals zero.

35
00:03:06,970 --> 00:03:13,630
Meaning if you're on step divisible by 100 every 100 steps we're going to do something every 100 steps

36
00:03:13,720 --> 00:03:22,560
we're going to report back our accuracy on the Tesa will say print will say on Stapp and then I can

37
00:03:22,570 --> 00:03:24,910
use that format here.

38
00:03:24,910 --> 00:03:31,700
So in a safe format I that I'm going to print accuracy.

39
00:03:34,830 --> 00:03:38,160
And let's go ahead and calculate our accuracy.

40
00:03:38,160 --> 00:03:45,870
We'll do the calculation essentially just like with last time will say T.F. equal T.F. RMX y pred come

41
00:03:45,870 --> 00:03:46,820
a 1.

42
00:03:46,860 --> 00:03:56,420
So this is the exact same calculation we did previously and will say T.F. RMX y true comma 1.

43
00:03:56,800 --> 00:04:02,220
OK so we have our matches remember that's a list of booleans So we want to cast it into a list of floats

44
00:04:02,280 --> 00:04:04,020
and then take the average.

45
00:04:04,080 --> 00:04:13,770
So accuracy isn't equal to T.F. reduce mean and I'm going to cast that list of booleans that is the

46
00:04:13,820 --> 00:04:28,990
matches into 32 perfect then it we'll say print the result of session run and I'm going to run that

47
00:04:29,020 --> 00:04:34,590
accuracy measurement in my feed dictionary is then going to be the test results or the test set.

48
00:04:34,590 --> 00:04:38,850
I should say so this test images.

49
00:04:39,090 --> 00:04:41,310
So those are actual images that I'm feeding in.

50
00:04:41,380 --> 00:04:44,710
And then why true is going to be honest.

51
00:04:45,040 --> 00:04:46,190
Test labels.

52
00:04:46,330 --> 00:04:51,340
Let me zoom out one level here so we can see the whole thing labels and then remember I still have one

53
00:04:51,340 --> 00:04:54,340
placeholder for hold probability and there is the testing.

54
00:04:54,340 --> 00:05:00,370
I don't want to actually drop neurons so I will say hold probability is 1.0.

55
00:05:00,470 --> 00:05:04,730
Basically no neuron is going to be dropped because they'll have a hundred percent chance of being held

56
00:05:06,130 --> 00:05:11,260
and there we're going to print a new line OK.

57
00:05:11,300 --> 00:05:12,710
Let's make sure we don't have any typos.

58
00:05:12,710 --> 00:05:14,410
Let's try to run this.

59
00:05:14,630 --> 00:05:15,270
And there we go.

60
00:05:15,290 --> 00:05:19,780
So you can see it's already started as you go and read out of the gate even just that 100 steps or you

61
00:05:19,790 --> 00:05:22,760
have 94 percent accuracy which is really good.

62
00:05:22,760 --> 00:05:26,600
And you can see here it's continuing and a 6 percent 97 is 7.

63
00:05:26,660 --> 00:05:30,270
So this isn't exactly state of the art but it's still pretty darned good.

64
00:05:30,770 --> 00:05:36,500
You can see here already approaching 98 after almost just two thousand steps.

65
00:05:36,500 --> 00:05:40,460
So hopefully if we keep running this long enough and we can play around with the size of the convolutions

66
00:05:40,850 --> 00:05:42,750
we'll get closer to 99 percent.

67
00:05:42,850 --> 00:05:44,590
So 98 98 etc..

68
00:05:44,600 --> 00:05:50,480
So you can see here a huge improvement by using convolutional Lares and pooling layers.

69
00:05:50,480 --> 00:05:51,050
All right.

70
00:05:51,110 --> 00:05:53,300
So I hope you enjoy that exercise eventually at the end.

71
00:05:53,300 --> 00:05:56,570
We got pretty close to 99 percent accuracy.

72
00:05:56,660 --> 00:05:59,240
If you have any questions feel free to post the Q Any forums.

73
00:05:59,300 --> 00:06:03,810
Coming up next is going to be your exercise for convolutional neural networks.

74
00:06:03,830 --> 00:06:04,730
I'll see you at the next lecture.

