1
00:00:06,090 --> 00:00:12,210
Welcome everyone to this lecture on an introduction to the precept Tron before we launch straight into

2
00:00:12,210 --> 00:00:19,100
neural networks we understand the individual components first such as a single neuron artificial neural

3
00:00:19,100 --> 00:00:22,590
networks or Amen's actually have a basis in biology.

4
00:00:22,820 --> 00:00:27,560
So we're going to see how we can attempt to mimic biological neurons with an artificial neuron otherwise

5
00:00:27,560 --> 00:00:28,850
known as a perception.

6
00:00:29,030 --> 00:00:33,290
And then once we go through the process of how a simple perception works we'll go ahead and show you

7
00:00:33,290 --> 00:00:36,370
how you can represent that mathematically.

8
00:00:36,470 --> 00:00:40,990
But let's start off of a biological neuron such as a brain cell.

9
00:00:41,000 --> 00:00:46,210
So the biological neuron works as in a simplified way through the following manner.

10
00:00:46,220 --> 00:00:49,650
Basically you have dendrites that feed into the body of this cell.

11
00:00:49,650 --> 00:00:54,380
And you can have many rights and what happens is an electrical signal gets passed through the dendrites

12
00:00:54,380 --> 00:01:00,290
to the body of the cell and then later on a single output or a single electrical signal is passed on

13
00:01:00,290 --> 00:01:04,090
through an axon to later on Connect to some other neuron.

14
00:01:04,220 --> 00:01:09,320
And that's the basic idea we have kind of these many input Erlick of electro signals through the dendrites

15
00:01:09,410 --> 00:01:15,580
goes through the body and then a single actual signal output through the axon So the artificial neuron

16
00:01:15,700 --> 00:01:21,450
also has inputs and outputs so we can attempt to mimic the biological neuron.

17
00:01:21,520 --> 00:01:24,200
So this simple model again is just known as a perception.

18
00:01:24,220 --> 00:01:26,200
In this case we have two inputs.

19
00:01:26,200 --> 00:01:29,780
So let's go ahead and see a simple example of how it can work.

20
00:01:29,800 --> 00:01:35,350
So we have two inputs and a single output and we start indexing at zero so we have input of zero input

21
00:01:35,370 --> 00:01:37,300
one.

22
00:01:37,490 --> 00:01:39,820
So the inputs can have values of features.

23
00:01:39,820 --> 00:01:43,880
So when you have your data set you're going to have various features and these features can be anything

24
00:01:43,880 --> 00:01:50,270
from how many rooms a house has or how dark an image is represented by some sort of pixel amount or

25
00:01:50,270 --> 00:01:55,190
some sort of Darkness number etc. So essentially Don't worry right now about what these numbers actually

26
00:01:55,190 --> 00:01:56,020
represent.

27
00:01:56,030 --> 00:01:59,400
Later on we're dealing with real data sets will have something more tangible.

28
00:01:59,450 --> 00:02:01,930
For right now these are just arbitrary number choices.

29
00:02:02,090 --> 00:02:04,310
But again we have input zero and Input 1.

30
00:02:04,310 --> 00:02:11,430
So again indexing starts at zero here and we're going to assign them values of let's say 12 and 4 then

31
00:02:11,430 --> 00:02:15,260
the next step is to have these inputs multiplied by some sort of way.

32
00:02:15,510 --> 00:02:22,520
So we have weight zero for input zero and weight one for Input 1 and typically the weights are actually

33
00:02:22,520 --> 00:02:25,220
initialized through some sort of random generation.

34
00:02:25,220 --> 00:02:28,590
So you just choose a random number for these weights.

35
00:02:28,710 --> 00:02:34,480
In this case we'll just go ahead and pretend that the random numbers chosen was 0.5 and negative 1.

36
00:02:34,500 --> 00:02:36,880
Again the numbers here are pretty much arbitrary.

37
00:02:36,960 --> 00:02:39,120
Really focus on the general process.

38
00:02:40,250 --> 00:02:43,750
So now these inputs are going to be multiplied by the weights.

39
00:02:43,840 --> 00:02:45,040
So that ends up looking like this.

40
00:02:45,040 --> 00:02:48,510
We have 12 times 0.5 or 1 1/2 and that equals six.

41
00:02:48,520 --> 00:02:54,470
And then we say four times that if one is equal to negative four the next step is to take these results

42
00:02:54,500 --> 00:02:59,930
of the inputs multiplied by their respective weights and pass them into an activation function.

43
00:03:00,930 --> 00:03:03,170
There's many activation functions to choose from.

44
00:03:03,180 --> 00:03:06,420
Now we're going to cover this in a lot more detail later on.

45
00:03:06,420 --> 00:03:11,240
For now our activation function is actually going to be very simple.

46
00:03:11,340 --> 00:03:13,120
We're going to say the following.

47
00:03:13,290 --> 00:03:19,800
If the some of the inputs is positive return one or output one if the sum of the inputs is negative

48
00:03:19,920 --> 00:03:21,090
then output zero.

49
00:03:22,420 --> 00:03:27,610
So in our case if we say six plus negative four that's the same thing as saying six minus four.

50
00:03:27,610 --> 00:03:29,490
So then we get to as a result.

51
00:03:29,500 --> 00:03:37,050
So since the sum of the inputs was positive the activation function returns 1 or outputs 1.

52
00:03:37,160 --> 00:03:41,130
So there's a possible issue here and the issue is the following.

53
00:03:41,210 --> 00:03:48,710
What is the original inputs started off as zero that any way you chose even if it was randomly chosen

54
00:03:48,790 --> 00:03:51,800
multiply it by the input would still result in zero.

55
00:03:51,860 --> 00:03:57,980
So if input 0 happened to be zero and if it happened to be zero as well instead of 12 and 4 then you

56
00:03:57,980 --> 00:03:58,840
could have a problem.

57
00:03:58,850 --> 00:04:05,160
You essentially always get out zero because zero multiplied by the way is still going to be zero.

58
00:04:05,180 --> 00:04:07,940
So we can actually fix this by adding in a biased term.

59
00:04:07,960 --> 00:04:09,650
And in this case we choose one.

60
00:04:09,860 --> 00:04:15,060
So we're going to go ahead and add in our own biased term here plus one.

61
00:04:15,070 --> 00:04:16,390
So what does this actually look like.

62
00:04:16,390 --> 00:04:22,230
Mathematically Now let's quickly think about how we can represent this perception model mathematically

63
00:04:22,410 --> 00:04:24,010
and it's actually quite simple.

64
00:04:24,030 --> 00:04:30,800
Basically we just say the following from equals zero to n that is the number of inputs we're gonna say.

65
00:04:30,810 --> 00:04:37,410
W If I were that specific wait for that input multiplied by XVI which is the input itself plus the bias

66
00:04:37,410 --> 00:04:37,780
term.

67
00:04:37,830 --> 00:04:40,350
And that's essentially it.

68
00:04:40,350 --> 00:04:46,810
So once we have many preset trons in a network we'll see how we can easily extend this to a matrix form.

69
00:04:46,840 --> 00:04:51,680
So as a quick review in this pretty simple lecture we just covered briefly the following.

70
00:04:51,820 --> 00:04:57,960
We discussed in the very basic terms how biological neuron works then how that can reflect to a perception

71
00:04:57,970 --> 00:05:03,710
model which is the artificial neuron and then the mathematical representation of that perception model.

72
00:05:03,850 --> 00:05:08,770
Coming up next we're going to continue discussing how we can build off this perception model to build

73
00:05:08,830 --> 00:05:09,880
a neural network.

74
00:05:10,090 --> 00:05:11,880
Thanks and I'll see at the next lecture.

