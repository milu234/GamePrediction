1
00:00:05,690 --> 00:00:08,780
Welcome back everyone to an introduction to neural networks.

2
00:00:10,290 --> 00:00:12,720
We've seen how a single percept Trump behaves.

3
00:00:12,720 --> 00:00:16,440
Now let's expand on this concept to get the idea of a neural network.

4
00:00:16,620 --> 00:00:22,890
Let's see how to connect many perceptive thoughts together and how to represent this mathematically.

5
00:00:22,900 --> 00:00:25,810
So what does a multiple perceptions that work actually look like.

6
00:00:25,810 --> 00:00:27,550
Well essentially looks like this.

7
00:00:27,610 --> 00:00:32,650
Here we can see that we have various layers of single percept trons connected to each other through

8
00:00:32,650 --> 00:00:35,040
their inputs and outputs.

9
00:00:35,060 --> 00:00:38,740
In this case we have an input layer all the way on the left which is purple.

10
00:00:38,780 --> 00:00:44,180
We have two hidden layers and hidden layers are the layers that are in-between the input layer and all

11
00:00:44,180 --> 00:00:49,370
the way on the right that output layer essentially hidden layers are layers that don't get to see the

12
00:00:49,370 --> 00:00:49,940
outside.

13
00:00:49,940 --> 00:00:56,130
That is all the way the inputs on the left or all the way the outputs on the right so input layers those

14
00:00:56,140 --> 00:01:00,710
are real values from the data so they take in the actual data as their input.

15
00:01:00,790 --> 00:01:04,850
Then you have hidden layers and those layers between the input output layers.

16
00:01:04,990 --> 00:01:10,930
And if you have three or more hidden layers that's considered a deep network and then an output layer

17
00:01:11,050 --> 00:01:17,590
where you have some sort of final estimate of whatever the output that you're trying to estimate is.

18
00:01:17,810 --> 00:01:22,620
So as you go forwards through more layers the level of abstraction increases.

19
00:01:22,760 --> 00:01:26,060
Let's not discuss the activation function in a little more detail.

20
00:01:28,070 --> 00:01:33,560
Previously our activation function was just a really simple function the output 0 or 1.

21
00:01:33,590 --> 00:01:37,940
Remember we were just taking the some of the inputs in that simple percept Tron model and then had an

22
00:01:38,000 --> 00:01:44,390
activation function that just had an output of either or 1 based off whether that number was positive

23
00:01:44,420 --> 00:01:45,180
or negative.

24
00:01:45,340 --> 00:01:51,680
And if we plotted out that looks like this we can say the output is either 0 or 1 on the y axis and

25
00:01:51,680 --> 00:01:54,410
then the x axis we see our marking at zero.

26
00:01:54,620 --> 00:02:00,980
And if we have a negative value we output 0 and if we have a positive value we output 1 and we did this

27
00:02:00,980 --> 00:02:04,390
by having the x axis B W X plus B.

28
00:02:04,550 --> 00:02:10,390
That is the wait times the inputs plus that bias and it's really common to just set that equal to Z.

29
00:02:10,490 --> 00:02:15,150
So you can refer to Z instead of having to constantly for two W X plus B.

30
00:02:15,170 --> 00:02:23,200
So whenever I say Z I'm essentially referring to those wait times those inputs plus that buys So unfortunately

31
00:02:23,200 --> 00:02:27,330
this is actually a pretty dramatic function since small changes aren't really reflected.

32
00:02:27,520 --> 00:02:35,390
So you can have kind of really small changes in Z for instance the change from 0.5 and Z to 0.6.

33
00:02:35,440 --> 00:02:39,310
Doesn't really matter it's still going to output one as long as it's positive or the changes that are

34
00:02:39,310 --> 00:02:43,130
really dramatic from negative one to negative thousand doesn't really matter.

35
00:02:43,150 --> 00:02:45,320
It's going to be zero.

36
00:02:45,640 --> 00:02:48,320
So it'd be nice if we could have a more dynamic function.

37
00:02:48,340 --> 00:02:54,630
For example this red line here you can see here that our outputs now are no longer just 0 or 1.

38
00:02:54,640 --> 00:02:56,550
We have a little more detail to it.

39
00:02:56,560 --> 00:02:58,890
There's a little more nuance here.

40
00:02:58,960 --> 00:03:01,300
So lucky for us this is actually the sigmoid function.

41
00:03:01,300 --> 00:03:05,340
This red line function already exists and it's known as the sigmoid function.

42
00:03:05,380 --> 00:03:13,030
And they say my function is just f of x is equal to 1 over 1 plus E to the negative x and we can replace

43
00:03:13,120 --> 00:03:14,760
X here with Z.

44
00:03:14,770 --> 00:03:17,440
So we have Z is equal to W X plus be.

45
00:03:17,440 --> 00:03:21,290
So we just take the whole thing that Z and plug it into the sigmoid function.

46
00:03:21,370 --> 00:03:26,280
So we'd have f of Z is equal to 1 over 1 plus it's the negative Z.

47
00:03:26,290 --> 00:03:32,760
And that would be our sieve function for the activation function so changing the activation function

48
00:03:32,790 --> 00:03:38,030
used can actually be really beneficial depending on the particular task we're trying to complete.

49
00:03:38,070 --> 00:03:42,830
Now we'll get into more detail on that later on when we actually implement our own that works.

50
00:03:44,040 --> 00:03:49,040
Let's discuss a few more activation of functions that we're going to encounter throughout this course.

51
00:03:49,110 --> 00:03:52,610
So a really common function that's used is an activation function.

52
00:03:52,620 --> 00:03:59,070
Besides just one function is the hyperbolic tinge of function or 10 H or tannish of Z or Z remembers

53
00:03:59,070 --> 00:04:05,850
just W X plus B and that looks really similar to the sigmoid function except note that the output can

54
00:04:05,850 --> 00:04:08,040
then be from negative 1 to 1.

55
00:04:08,130 --> 00:04:10,370
Unlike Previously we had zero to 1.

56
00:04:10,440 --> 00:04:16,020
So if we look at the Sigma function again that went from 0 to 1 and that was equal to 1 over 1 plus

57
00:04:16,080 --> 00:04:19,300
it's a negative x or negative Z whatever the input is.

58
00:04:19,320 --> 00:04:25,290
And now for the Tench or hyperbolic tangent function it goes from negative 1 to 1 but still has it sort

59
00:04:25,290 --> 00:04:26,690
of same behavior shape.

60
00:04:26,760 --> 00:04:31,830
And if you're interested in how to express the hyperbolic tangent mathematically over here on the right

61
00:04:31,830 --> 00:04:40,800
hand side or the actual equations for then there's also the rectified linear unit or r e l u.

62
00:04:41,360 --> 00:04:45,610
And this is a really common activation function but it's actually relatively simple.

63
00:04:45,710 --> 00:04:52,340
And the way it works is the following method you basically passing your z value into this Max equation.

64
00:04:52,350 --> 00:04:55,160
So you say max of 0 or Z.

65
00:04:55,460 --> 00:04:56,720
So what does that actually mean.

66
00:04:56,720 --> 00:05:01,370
Well the rectified linear unit that actual activation function works the following manner.

67
00:05:01,520 --> 00:05:04,720
It says based off your z value and zero.

68
00:05:04,790 --> 00:05:11,900
Just return the maximum value and that actually means that if your value of z is negative then the max

69
00:05:11,900 --> 00:05:15,030
between zero and any negative number is always going to be zero.

70
00:05:15,230 --> 00:05:17,950
So we can see here that is z.

71
00:05:17,990 --> 00:05:26,720
If it's negative the function always returns 0 and then between Max 0 comma's Z if z is positive it's

72
00:05:26,720 --> 00:05:28,500
always going to return the value of z.

73
00:05:28,670 --> 00:05:31,980
So essentially of just y is equal to Z here.

74
00:05:32,090 --> 00:05:35,920
So on the right hand side that's where you get that sort of straight linear line.

75
00:05:35,930 --> 00:05:41,240
So again relatively simple function but this is kind of considered at certain points in time state of

76
00:05:41,240 --> 00:05:41,990
the art.

77
00:05:42,020 --> 00:05:48,590
So we're going to definitely be seeing this activation function a lot throughout our lecturers so that

78
00:05:48,590 --> 00:05:53,090
rectified linear unit and hyperbolic tangent tend to have the best performance so we're going to really

79
00:05:53,090 --> 00:05:58,230
focus on these two throughout the course but deep libraries in general have these built in force.

80
00:05:58,250 --> 00:06:02,620
So we don't really need to worry about having to implement these activation functions manually.

81
00:06:02,660 --> 00:06:05,970
We'll essentially be able to select them from our deep learning library.

82
00:06:07,290 --> 00:06:11,280
As we continue on we'll also talk about some more State of the activation functions.

83
00:06:11,300 --> 00:06:16,560
But up next we're going to do is discuss cost functions which will allow us to measure how well these

84
00:06:16,560 --> 00:06:18,590
neurons are actually performing.

85
00:06:18,600 --> 00:06:24,810
So right now we've understood two major concepts the perception and then the activation function inside

86
00:06:24,810 --> 00:06:26,340
that particular neuron.

87
00:06:26,340 --> 00:06:28,350
Up next we'll discuss cost functions.

88
00:06:28,530 --> 00:06:29,950
Thanks and I'll see it there.

