1
00:00:06,410 --> 00:00:08,750
Welcome back everyone in this lecture.

2
00:00:08,750 --> 00:00:15,970
We're going to discuss cost functions we'll now explore how can we evaluate the performance of a neuron

3
00:00:16,540 --> 00:00:22,370
we can use a cost function to measure how far off we are from the expected value.

4
00:00:22,420 --> 00:00:25,060
So we're going to use the following variables and notation.

5
00:00:25,060 --> 00:00:29,980
Let's quickly review this so you don't get confused when we actually see the cost functions we'll use

6
00:00:29,980 --> 00:00:36,940
the variable y to represent the true value and we'll use the variable A to represent the neurons prediction.

7
00:00:36,940 --> 00:00:39,360
So what does this actually mean in terms of weights and biases.

8
00:00:39,370 --> 00:00:46,020
Well remember that we said W times x that is the wait times the input plus the bias is equal to Z.

9
00:00:46,120 --> 00:00:49,900
And remember that we pass Z into the activation function.

10
00:00:49,900 --> 00:00:56,430
So for example if we're using the sigmoid function we could say that sigma of Z is equal to A.

11
00:00:56,650 --> 00:01:03,170
So that neurons prediction when we actually put in the some of those weights times the inputs plus those

12
00:01:03,170 --> 00:01:09,130
biased terms we get the Z and then we can pass that in to the activation function sigma of Z and that

13
00:01:09,220 --> 00:01:09,980
is equal to A.

14
00:01:10,030 --> 00:01:14,470
Which at the end of represents the neurons prediction.

15
00:01:14,490 --> 00:01:20,010
So the first cost function we're going to discuss is the quadratic cost function and that is equal to

16
00:01:20,400 --> 00:01:25,280
the sum of y minus a squared divided by.

17
00:01:25,500 --> 00:01:27,510
And you can if you want pull out one over.

18
00:01:27,510 --> 00:01:30,120
And so what do we actually see here.

19
00:01:30,120 --> 00:01:35,730
Well we can see that larger errors are more prominent due to squaring because remember why is that true

20
00:01:35,730 --> 00:01:37,890
value and A's are predictive value.

21
00:01:38,040 --> 00:01:42,930
So if there's a big difference there when we square it's going to really be prominent now.

22
00:01:43,020 --> 00:01:48,180
Unfortunately when you actually implement this quadratic costs function this coloration could cause

23
00:01:48,180 --> 00:01:53,590
a slowdown in our learning speed to the way that formulaically it actually works.

24
00:01:54,400 --> 00:01:58,400
So instead we're going to be using cross entropy as our cost function.

25
00:01:58,570 --> 00:02:04,810
And this is defined in the following manner we say negative 1 over and and then we take the sum of Y

26
00:02:04,900 --> 00:02:10,910
times the log a plus one minus Y times the log of 1 minus a.

27
00:02:10,960 --> 00:02:14,740
And this actual cost function allows for faster learning.

28
00:02:14,770 --> 00:02:17,100
And the reason for that is the following.

29
00:02:17,110 --> 00:02:23,320
The larger the difference between y and a D faster than your on ends up actually learning due to this

30
00:02:23,320 --> 00:02:25,900
cross and should be cost function.

31
00:02:25,900 --> 00:02:30,880
So that means that if we end up at the beginning for really large difference between our predicted value

32
00:02:31,120 --> 00:02:37,090
and the true value we can essentially quickly jump forward using this cost function because the larger

33
00:02:37,090 --> 00:02:40,910
the difference the faster Nero is going to learn.

34
00:02:41,050 --> 00:02:46,420
So now that we have these two key aspects of learning of general networks the neurons of their activation

35
00:02:46,420 --> 00:02:48,160
function and the cost function.

36
00:02:48,280 --> 00:02:52,960
We're still missing a key step and that's the actual learning process.

37
00:02:53,050 --> 00:02:58,930
So we need to figure out how we can use our neurons and the measurements of our air our cost function

38
00:02:58,960 --> 00:03:02,920
basically a measurement of how wrong we are and then attempt to correct our prediction.

39
00:03:02,920 --> 00:03:04,900
In other words actually learn.

40
00:03:05,250 --> 00:03:11,260
So again so far we understand the neurons the precept trons how we can link them together to get a neural

41
00:03:11,260 --> 00:03:16,150
net and then we also understand now that we have these cost functions that are essentially measurements

42
00:03:16,150 --> 00:03:20,070
of how off we are how wrong we are what our error in general is.

43
00:03:20,290 --> 00:03:25,970
So now that we have these two components we need to understand well how to actually fix that error and

44
00:03:26,000 --> 00:03:30,290
we're going to do in the next lecture we're going to briefly cover how we can do this with gradient

45
00:03:30,290 --> 00:03:32,630
descent which is probably a term you've heard before.

46
00:03:32,630 --> 00:03:34,790
If you've ever read anything on machine learning.

47
00:03:35,100 --> 00:03:38,410
OK let's go ahead and discuss gradient descent in the next lecture.

48
00:03:38,450 --> 00:03:39,110
I'll see it there.

