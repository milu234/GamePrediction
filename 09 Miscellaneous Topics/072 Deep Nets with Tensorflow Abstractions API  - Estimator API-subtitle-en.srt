1
00:00:05,330 --> 00:00:10,340
Welcome back everyone in this lecture we're going to show you how to use the estimator API in order

2
00:00:10,340 --> 00:00:13,610
to solve the classification problem for that wind data set.

3
00:00:13,610 --> 00:00:15,300
Let's go back to Jupiter notebook.

4
00:00:15,500 --> 00:00:17,530
OK so we have our wind data set.

5
00:00:17,540 --> 00:00:24,420
The next thing I will do in order to start using the estimator API is they will import sensor flow as

6
00:00:24,420 --> 00:00:36,530
T.F. and then I also say from sensor flow in poor estimator and then I will go ahead and run that.

7
00:00:36,640 --> 00:00:41,950
OK so as a reminder if we take a look at our X train shape that 70 percent of the data.

8
00:00:41,970 --> 00:00:47,210
So we have 124 rows with 13 columns have the features.

9
00:00:47,210 --> 00:00:50,030
And then we have the additional label column.

10
00:00:50,030 --> 00:00:55,820
So to use the S-meter API which gives them before I need to create feature columns and if they're all

11
00:00:55,820 --> 00:01:00,180
numeric columns which if we take a look at x train that's actually the case here.

12
00:01:00,200 --> 00:01:01,370
So take a look at x train.

13
00:01:01,370 --> 00:01:04,060
These are all just numeric feature columns.

14
00:01:04,070 --> 00:01:12,000
What I can do to quickly create a list of just a bunch of numeric columns is say C-f feature column

15
00:01:12,600 --> 00:01:18,180
numeric column and then I going to pass in the string x to represent the actual features and then a

16
00:01:18,250 --> 00:01:25,120
pass a shape attribute to represent the actual 13 data features.

17
00:01:25,130 --> 00:01:29,390
So that's a really quick way to create feature columns if all your columns happen to be numeric.

18
00:01:29,390 --> 00:01:32,900
We've shown you before how to deal with category calls with the estimator API.

19
00:01:32,900 --> 00:01:36,350
So you can check back on those videos in case you need a refresher.

20
00:01:36,350 --> 00:01:42,810
Up next is actually straight to the model so create a variable called Deep underscore model and then

21
00:01:42,840 --> 00:01:48,240
off the estimator API if you do that here and then hit tab you can see the various functions that are

22
00:01:48,240 --> 00:01:49,230
available to us.

23
00:01:49,230 --> 00:01:56,560
We're going to do then Cely connected or a deep neural network classifier and then we have this hidden

24
00:01:56,560 --> 00:01:58,560
units argument which is just the list.

25
00:01:58,690 --> 00:02:01,930
And you basically say how many neurons you want in each layer.

26
00:02:01,930 --> 00:02:06,790
So in our case we're just going to say 13:13 13 if you start doing more than that you'll probably be

27
00:02:06,790 --> 00:02:08,980
overfitting especially for such a small data set.

28
00:02:09,130 --> 00:02:13,120
But if you're looking for really large dataset maybe you'll need more than you can adjust as you see

29
00:02:13,120 --> 00:02:13,590
fit.

30
00:02:14,600 --> 00:02:20,700
And the next one is to find the actual feature columns will say feature columns is equal to that feat

31
00:02:20,700 --> 00:02:25,980
calls variable we just made and then we also need to say the number of classes we're predicting.

32
00:02:26,130 --> 00:02:27,920
So by default it's binary classification.

33
00:02:27,930 --> 00:02:29,960
But in our case we have three different types.

34
00:02:30,030 --> 00:02:34,810
So we'll go ahead and say there's three different classes there and then we can also pass in an optimizer.

35
00:02:34,830 --> 00:02:45,930
So we'll pass A.F. train and we'll do a gradient optimizer with the learning rates equal to 0.01 OK

36
00:02:45,930 --> 00:02:51,000
so run that and you should be able to see your model created and then it's actually really easy to train

37
00:02:51,000 --> 00:02:51,860
your model.

38
00:02:51,870 --> 00:02:55,030
The only thing left we need to build out is our input function.

39
00:02:55,530 --> 00:03:00,600
Again we've shown you before how to create these input functions you just call estimator inputs and

40
00:03:00,600 --> 00:03:05,520
then you have two options one for an umpire function one for panderers murthering of just not pi.

41
00:03:05,520 --> 00:03:10,050
So we'll say x is equal to the x variable.

42
00:03:10,050 --> 00:03:13,740
We said earlier is kind of almost like a fit dictionary here.

43
00:03:13,920 --> 00:03:21,030
Scaled X train then the y component is equal to y train.

44
00:03:21,030 --> 00:03:24,830
And since we're training all this will say shuffle is equal to true.

45
00:03:25,320 --> 00:03:27,750
And then we can define a batch size.

46
00:03:27,970 --> 00:03:33,100
I'll go ahead and say 10 for this level data and then we can find the number of POCs how many times

47
00:03:33,100 --> 00:03:39,980
you want to run through this entire training set and we'll go on to say five run that and then let's

48
00:03:39,980 --> 00:03:43,160
go ahead and trayner deep model on that input function.

49
00:03:43,160 --> 00:03:50,220
So I will say input function here and let's run this and we can also if we want to or we can say steps

50
00:03:50,220 --> 00:03:51,400
here if necessary.

51
00:03:51,570 --> 00:03:54,370
So I could say something like subscales 500.

52
00:03:54,460 --> 00:03:55,530
So go ahead and run that.

53
00:03:55,570 --> 00:03:59,130
It's going to create that and then it will train it should be pretty straightforward.

54
00:03:59,500 --> 00:04:03,430
And then we're also going to create an evaluation function which is going to look really similar to

55
00:04:03,430 --> 00:04:09,920
our previous function again estimator inputs from PI input function.

56
00:04:10,240 --> 00:04:15,750
But in this case we're just going to be passing in our test data.

57
00:04:15,940 --> 00:04:23,430
So this will be scaled X test scale x test and then I also want to make sure that shuffle is equal to

58
00:04:23,430 --> 00:04:24,420
false.

59
00:04:24,420 --> 00:04:30,900
So don't shuffle my testate around run that and then all say predictions is equal to.

60
00:04:30,900 --> 00:04:34,760
And if you say deep model you can call the Predict method off of this.

61
00:04:34,770 --> 00:04:38,630
Keep in mind it also has if you'll remember and evaluate method.

62
00:04:38,700 --> 00:04:42,030
So I could pass in the actual test labels.

63
00:04:42,050 --> 00:04:46,740
But in this case we're going to do that manually which it predicts and the input function is going to

64
00:04:46,740 --> 00:04:53,050
be equal to input function evil evil remember and recall that this is actually a generator so if you

65
00:04:53,050 --> 00:04:57,850
want to get the actual results you know cast this something or loop through it later on.

66
00:04:57,850 --> 00:05:01,030
So we'll cast this into a list.

67
00:05:01,040 --> 00:05:05,820
So now if I take a look at Pretz it's this list of dictionary objects.

68
00:05:05,840 --> 00:05:11,930
So I will use some list comprehension to just grab what I'm looking for which is this array value the

69
00:05:11,930 --> 00:05:21,210
class ID so that predictions is equal to an say P class ID.

70
00:05:21,210 --> 00:05:23,530
So that's the dictionary inside there.

71
00:05:23,680 --> 00:05:27,390
And then I also want to grab the actual results here so that 0.

72
00:05:27,480 --> 00:05:34,380
So will say X off of that 0 and say for P and Pretz.

73
00:05:34,580 --> 00:05:35,330
So run that.

74
00:05:35,450 --> 00:05:39,320
And that's just a simple list comprehension that eventually just gets you back that list the actual

75
00:05:39,320 --> 00:05:42,080
classes predicted zeros ones or twos.

76
00:05:42,410 --> 00:05:51,170
And in order to evaluate this we're going to do is say from S-K learn the metrics import and you can

77
00:05:51,170 --> 00:05:52,550
do either confusion matrix.

78
00:05:52,550 --> 00:05:54,580
You can also do it classification report.

79
00:05:54,830 --> 00:06:01,930
Let's go in and just do a classification report here and we'll pass in our test and our predictions

80
00:06:03,300 --> 00:06:06,290
and let's print this up.

81
00:06:06,720 --> 00:06:09,770
OK so running that you should have some pretty good results for that.

82
00:06:09,960 --> 00:06:12,000
Hovering around above 85.

83
00:06:12,000 --> 00:06:17,400
So in this case I performed pretty well in 95 94 94 but that's the estimator API.

84
00:06:17,520 --> 00:06:19,380
You've already used that before in the beginning of the course.

85
00:06:19,380 --> 00:06:21,400
Hopefully that was basically all of you for you.

86
00:06:21,570 --> 00:06:24,630
Let's quickly do a quick rundown of everything we did.

87
00:06:24,630 --> 00:06:26,900
First things first you have to get your data in the right shape.

88
00:06:26,910 --> 00:06:31,350
Once you've done that you can go ahead and create your feature columns feature columns can be a little

89
00:06:31,350 --> 00:06:36,800
more complicated if you have categorical data but again recall that you can just call tab here and there's

90
00:06:36,810 --> 00:06:38,630
different versions of categorical data.

91
00:06:38,640 --> 00:06:41,520
We covered those in the beginning of the course using the estimator API.

92
00:06:41,670 --> 00:06:46,600
But if you have all numeric columns this is a nice little feature with the shape then we create a classifier

93
00:06:47,010 --> 00:06:48,330
defining the hidden units.

94
00:06:48,320 --> 00:06:51,400
You can easily add more layers just by making this larger.

95
00:06:51,540 --> 00:06:55,730
You have the feature columns and the Reclast to predict the specific optimizer with your learning right

96
00:06:56,980 --> 00:07:01,860
then you create these input functions which you like to basically act like a feed dictionary inputting

97
00:07:01,870 --> 00:07:02,410
that data.

98
00:07:02,440 --> 00:07:06,730
And if you're training you also want to make sure you pass and your training labels you want to shuffle

99
00:07:06,730 --> 00:07:11,260
this and you can find your batch size number of pocks you train your model all the data and then you

100
00:07:11,260 --> 00:07:16,510
can evaluate it and use predict or evaluation the Pennine what your estimate or input function looks

101
00:07:16,510 --> 00:07:17,250
like.

102
00:07:17,300 --> 00:07:17,690
OK.

103
00:07:17,740 --> 00:07:19,330
That's really all there is to it.

104
00:07:19,330 --> 00:07:23,670
Coming up next we're going to discuss the keris abstraction off of Tancer flow.

105
00:07:23,700 --> 00:07:24,480
I'll see it there.

