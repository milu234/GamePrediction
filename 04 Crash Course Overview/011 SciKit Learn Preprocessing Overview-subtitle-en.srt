1
00:00:05,290 --> 00:00:06,030
Welcome everyone.

2
00:00:06,040 --> 00:00:10,930
This quick overview on psychist learn and we really won't be talking about sikat learn machine learning

3
00:00:10,930 --> 00:00:13,830
models because we're going to be using tent's flow instead.

4
00:00:14,140 --> 00:00:19,270
Well we're going to be talking about in this lecture is using sikat learn for pre-processing and specifically

5
00:00:19,270 --> 00:00:25,050
two things and that is scaling our data and also splitting our data into train test sets.

6
00:00:25,060 --> 00:00:29,260
Let's go ahead and jump to DUPERE notebook and see the convenience functions that sikat learned provides

7
00:00:29,260 --> 00:00:30,250
for us.

8
00:00:30,250 --> 00:00:30,640
OK.

9
00:00:30,680 --> 00:00:32,050
Go ahead and generate some data.

10
00:00:32,050 --> 00:00:39,460
I will import umpire as an p and then we're mainly going to be using preprocessing of sikat learn so

11
00:00:39,490 --> 00:00:43,020
say from S-K learn pre-processing import.

12
00:00:43,090 --> 00:00:46,350
And we'll be importing the minimax scaler.

13
00:00:46,810 --> 00:00:48,950
Let's go ahead and create some data we'll just say.

14
00:00:48,950 --> 00:00:51,990
And we ran them ran in.

15
00:00:52,030 --> 00:00:53,920
And we'll say random data.

16
00:00:53,950 --> 00:00:55,130
Zero to 100.

17
00:00:55,450 --> 00:00:58,430
And we'll have it be 10 rows by two columns.

18
00:00:58,450 --> 00:01:04,180
So if I take a look at my data I see here that I have the 10 rows two columns kind of think of this

19
00:01:04,480 --> 00:01:07,880
as either features in a dataset or labels in a dataset etc..

20
00:01:08,970 --> 00:01:15,150
So if I actually want to run this data into a neural network usually what I should be doing is scaling

21
00:01:15,150 --> 00:01:16,960
that data in order to do that.

22
00:01:16,980 --> 00:01:20,110
I can use the minimax scalar.

23
00:01:20,200 --> 00:01:24,380
So the way we do this is we create an instance of that minimax scalar.

24
00:01:24,460 --> 00:01:26,500
So I'll say scaler underscore model.

25
00:01:26,540 --> 00:01:30,840
Really whatever you want to call it minimax scaler close print sees.

26
00:01:30,880 --> 00:01:34,440
And now I check out the type of that scalar model.

27
00:01:34,510 --> 00:01:37,530
It is a instance of a min max scalar.

28
00:01:37,540 --> 00:01:41,280
And then what we do is offer this scalar model.

29
00:01:41,590 --> 00:01:46,690
We're going to fit it to our data and fitting it to her data.

30
00:01:46,900 --> 00:01:53,310
Basically allows the model to learn what the minimum value and what the maximum value is for each column.

31
00:01:53,380 --> 00:01:55,610
This warning right here you don't need to worry about it too much.

32
00:01:55,630 --> 00:02:00,580
It's just reporting back that it's converting the integers into floating point numbers which makes sense

33
00:02:00,580 --> 00:02:04,840
because we expect these wants her scale to be floating point numbers.

34
00:02:04,960 --> 00:02:11,400
Then once we do that we can go ahead and grab her model and then transform the data and then once you

35
00:02:11,400 --> 00:02:16,890
transform data you'll notice what happens is that the minimum value becomes zero and the maximum value

36
00:02:16,890 --> 00:02:19,750
becomes one and that goes across each column.

37
00:02:19,920 --> 00:02:24,600
So we kind of scroll up here we can see here that in my case with this random data I had a max value

38
00:02:24,600 --> 00:02:28,250
for the first column at 92 and the minimum value was three.

39
00:02:28,260 --> 00:02:34,070
So if we divide each value in this column by the max 92 we end up getting these results.

40
00:02:34,080 --> 00:02:36,520
And that that is what the min max scaler does.

41
00:02:36,540 --> 00:02:38,480
There's other ways of normalizing your data.

42
00:02:38,490 --> 00:02:43,560
But for most of our cases since we're dealing with pretty basic data sets the min max scaler will be

43
00:02:43,560 --> 00:02:44,320
enough.

44
00:02:44,470 --> 00:02:49,770
Now technically I could have done this in one command instead of saying fit the data and then transform

45
00:02:49,770 --> 00:02:55,110
the data the scalar model also has a fit transform.

46
00:02:55,470 --> 00:03:02,280
And if you run that on the data it performs both those steps at once depending on what situation you're

47
00:03:02,280 --> 00:03:07,950
in where you're usually going to do is you're going to fit to your training data and then transform

48
00:03:07,950 --> 00:03:10,660
your training data and then transform your test data.

49
00:03:10,680 --> 00:03:15,420
And the reason for that is because you don't really want to cheat by fitting to your test data as well

50
00:03:15,420 --> 00:03:20,280
as your trained data because you don't want to assume that you're going to know what your test data

51
00:03:20,280 --> 00:03:21,180
is going to look like.

52
00:03:21,180 --> 00:03:26,010
So typically you fit to your training data and then you transform to your test data and your training

53
00:03:26,010 --> 00:03:26,580
data.

54
00:03:26,670 --> 00:03:31,640
But the model itself this scaling model has only been fitted to your training data.

55
00:03:31,650 --> 00:03:38,610
All right let's go ahead and kind of show an example of a train test split so import Pandurs for this

56
00:03:39,540 --> 00:03:46,550
and would create a data frame so we can imagine we just read in some see as we file so we can say PD

57
00:03:46,630 --> 00:03:55,130
that data frames create a data frame and I'm going to say the data is equal to p that ran them ran Diante

58
00:03:55,300 --> 00:03:56,600
I'll say zero.

59
00:03:57,590 --> 00:03:58,240
101.

60
00:03:58,280 --> 00:04:00,450
And then 50 by four.

61
00:04:00,500 --> 00:04:08,260
In effect let's actually just grab that entire thing and put that in and you sell and will have that

62
00:04:08,260 --> 00:04:09,620
be my data.

63
00:04:10,000 --> 00:04:17,330
So if we take a look at my data it's just these integers here it's four columns by 50 rows.

64
00:04:17,530 --> 00:04:20,690
So scrolling down here that's going to be the data pasan.

65
00:04:21,040 --> 00:04:25,040
And you could put the whole command in one line but this makes it a little cleaner here.

66
00:04:25,150 --> 00:04:32,600
And then let's go ahead and if I just pass this in and I take a look at the data and if you want to

67
00:04:32,630 --> 00:04:34,670
call this DFA that it's a little clear.

68
00:04:34,940 --> 00:04:37,530
So I'll say DFAC could repeat the data from there.

69
00:04:37,540 --> 00:04:39,070
And if I take a look at the f.

70
00:04:39,200 --> 00:04:43,550
So now that I'm viewing my data frame I can see here that pair of those has auto named these columns

71
00:04:43,550 --> 00:04:46,000
to be 0 1 2 3.

72
00:04:46,010 --> 00:04:51,240
What it can do to make sure is columns have names is I can provide a list of names.

73
00:04:51,290 --> 00:04:53,340
So let's imagine this as a data set.

74
00:04:53,600 --> 00:04:59,350
So all have three features F1 F2 F3 and then some sort of label.

75
00:04:59,390 --> 00:05:01,790
So I can already tell this is a supervised learning problem.

76
00:05:01,790 --> 00:05:05,160
I have three features and I'm trying to predict this label.

77
00:05:05,180 --> 00:05:09,410
So now if I take a look at my data frame now the columns are named so three features and then the label

78
00:05:09,410 --> 00:05:10,790
column.

79
00:05:10,790 --> 00:05:12,900
Now let's imagine the Zura entire dataset.

80
00:05:12,980 --> 00:05:14,930
And I want to split it into a training set.

81
00:05:14,990 --> 00:05:19,060
And the testing set the way can do that is through the following method.

82
00:05:19,060 --> 00:05:26,050
If I scroll down here I can say my X that is my features is equal to data.

83
00:05:26,210 --> 00:05:33,100
And then I pass on a list of the columns only the feature columns that is F1 F2 F3.

84
00:05:33,170 --> 00:05:34,900
So what's the mixture.

85
00:05:34,910 --> 00:05:38,120
I personally fear there we go.

86
00:05:38,120 --> 00:05:43,550
So now if I take a look at X it's only the feature columns and I'm going to do a similar process for

87
00:05:43,550 --> 00:05:48,580
y for the labels will just say Y is equal to the F label.

88
00:05:49,410 --> 00:05:54,260
Run that then take a look at why it's just that label column.

89
00:05:54,260 --> 00:05:59,820
OK so now that I have my features my actual data and then the label that I'm trying to predict I can

90
00:05:59,820 --> 00:06:10,620
do a train test split seal say from S-K learn the model selection import train test split run that and

91
00:06:10,620 --> 00:06:16,440
then what I like to do is call train to split through shift tab here and then you can expand this to

92
00:06:16,440 --> 00:06:19,370
see the actual documentation.

93
00:06:19,380 --> 00:06:23,250
And if you scroll all the way down there's a nice little example you can kind of copy and paste from.

94
00:06:23,400 --> 00:06:29,400
And it's this one right here X train accessed y train and y to us copy that whole commands and then

95
00:06:29,400 --> 00:06:33,340
we can actually overwrite it here and put this all on one line.

96
00:06:33,450 --> 00:06:36,020
When we zoom out a little bit so we can see the whole thing.

97
00:06:36,060 --> 00:06:38,160
So it is trained to split actually do.

98
00:06:38,220 --> 00:06:42,300
Well you pass in your feature columns and then you press in your label.

99
00:06:42,450 --> 00:06:45,250
And then it's going to ask you what the test size is going to be.

100
00:06:45,420 --> 00:06:49,410
So it's asking you what percentage of the data you want to go to the test set.

101
00:06:49,440 --> 00:06:55,080
A really common value here is 30 percent or 33 percent should be your test 70 percent should be your

102
00:06:55,080 --> 00:06:55,890
training.

103
00:06:56,310 --> 00:07:00,980
And again this is a very situation specific number so there is no right or wrong answer here.

104
00:07:00,990 --> 00:07:03,130
Sometimes a 50/50 split makes sense.

105
00:07:03,300 --> 00:07:06,450
And then they ran them state is basically for repeatability.

106
00:07:06,450 --> 00:07:08,940
Just like we had some PI random set seed.

107
00:07:09,060 --> 00:07:10,820
We can do a seed here as well.

108
00:07:10,830 --> 00:07:16,700
That way you can always make sure that you get the same random splits so you can say you want to 1:42

109
00:07:16,780 --> 00:07:17,150
cetera.

110
00:07:17,190 --> 00:07:18,090
It's up to you.

111
00:07:18,240 --> 00:07:23,100
And then you get back these four variable results you get back in X train in excess a white train and

112
00:07:23,100 --> 00:07:24,560
a white test.

113
00:07:24,570 --> 00:07:32,460
So if I run this and I check out the shapes of these guys leps X train check out the shape.

114
00:07:32,460 --> 00:07:35,520
Notice I have 35 rows by three.

115
00:07:35,520 --> 00:07:37,990
So that means this is the actual x train.

116
00:07:38,070 --> 00:07:41,270
So that is the feature data for the training set.

117
00:07:41,400 --> 00:07:44,230
And if I take a look at X test.

118
00:07:44,280 --> 00:07:46,440
Well let's go ahead and take a look at the shape here.

119
00:07:48,320 --> 00:07:50,040
Then I have 15 by three.

120
00:07:50,060 --> 00:07:53,890
Notice that it's smaller because this is the feature for the test set.

121
00:07:53,900 --> 00:07:59,450
So the basic idea here would be once I actually have my neural network model working intenser flow and

122
00:07:59,450 --> 00:08:05,870
I want to do some sort of training process for supervised learning I would feed in the training sets

123
00:08:05,870 --> 00:08:12,980
for X train and y train and the model would try to basically build some sort of understanding of how

124
00:08:12,980 --> 00:08:20,540
the X training features are able to predict the why training labels once they have that then I can evaluate

125
00:08:20,540 --> 00:08:26,330
my model by feeding it the X test data and then it will try to predict what those labels should be in

126
00:08:26,330 --> 00:08:27,980
order to do the full evaluation.

127
00:08:27,980 --> 00:08:33,920
I can then compare those predictive values to the true test values and that's the reason for a train

128
00:08:33,920 --> 00:08:34,700
to split.

129
00:08:34,940 --> 00:08:36,860
And we're going to see this later on the course.

130
00:08:37,140 --> 00:08:37,580
OK.

131
00:08:37,760 --> 00:08:39,790
That's really all we need to know about sikat learn.

132
00:08:39,920 --> 00:08:45,080
And as a quick note sikat learn is actually its own machine learning library for Python it's one of

133
00:08:45,080 --> 00:08:50,570
most popular out there but it doesn't support the deep neural networks that tensor flow can do so which

134
00:08:50,570 --> 00:08:55,370
is why we're not really going to be using it in this course if you're interested in some of those other

135
00:08:55,400 --> 00:08:57,420
sikat learn machine learning model methods.

136
00:08:57,470 --> 00:09:01,340
You can check out that Python for data science and machine learning boot camp course.

137
00:09:01,340 --> 00:09:01,770
OK.

138
00:09:01,910 --> 00:09:03,890
Thanks everyone and I will see you at the next lecture.

