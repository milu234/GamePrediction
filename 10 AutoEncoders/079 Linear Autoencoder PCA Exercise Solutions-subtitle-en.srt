1
00:00:05,350 --> 00:00:06,380
Welcome back everyone.

2
00:00:06,400 --> 00:00:11,500
The solutions lecture for the dimensionality reduction exercise for linear audio and encoders.

3
00:00:11,530 --> 00:00:14,360
Let's go to the Jupiter notebook and walk through the solutions.

4
00:00:14,680 --> 00:00:16,270
OK so here we are at the notebook.

5
00:00:16,390 --> 00:00:21,400
We'll start off with our imports will the numb pie and then we're also going to use pandas to read and

6
00:00:21,400 --> 00:00:22,840
to see as we file.

7
00:00:22,840 --> 00:00:28,570
And then finally we're going to import map plot lived that pipe plot as PLDT and then we'll say map

8
00:00:28,570 --> 00:00:32,010
plot live in line OK.

9
00:00:32,030 --> 00:00:36,710
First things first we need to actually get our data will say DMF or whatever you want to call your data

10
00:00:36,710 --> 00:00:39,220
frame is equal to read CXXVIII.

11
00:00:39,380 --> 00:00:43,730
And since I'm already located in the auto encoder folder there's going to tab autocomplete for that

12
00:00:43,730 --> 00:00:45,160
CSP dataset.

13
00:00:45,260 --> 00:00:48,920
And if you take a look at the head of it it's going to look something like this.

14
00:00:48,950 --> 00:00:53,900
Basically you have these 30 columns of data and if you go all the way to the right you have that label

15
00:00:53,900 --> 00:00:57,460
column and the hope here that's just the result of calling info.

16
00:00:57,470 --> 00:01:01,970
It's another way of seeing general information but your data frame specifically about each of the columns

17
00:01:01,970 --> 00:01:06,920
so we can see here they're all basically the same thing just five hundred entries of numbers and all

18
00:01:06,920 --> 00:01:09,310
the way down and we have that label column.

19
00:01:09,320 --> 00:01:10,810
Now we want to scale the data.

20
00:01:11,080 --> 00:01:14,930
An important thing to note here when we're scaling data is we're not doing any sort of train test but

21
00:01:15,320 --> 00:01:16,930
because this is unsupervised learning.

22
00:01:16,940 --> 00:01:24,170
We just really care about the features we'll say from a scalar and pre-processing import and will improve

23
00:01:24,220 --> 00:01:31,640
the minimax scaler will create an instance of a min max scaler and then we'll finally say our scale

24
00:01:31,640 --> 00:01:34,310
data is equal to scaler.

25
00:01:34,370 --> 00:01:38,780
And again we'll just call it transform on our entire features dataset.

26
00:01:38,890 --> 00:01:47,350
So we'll say the drop the label and say x is equal to 1 OK.

27
00:01:47,380 --> 00:01:48,750
Now we have our scale data.

28
00:01:48,760 --> 00:01:50,410
So those are just the features.

29
00:01:50,410 --> 00:01:55,090
Up next we want to actually begin to build out a real linear auto encoder which means we're going to

30
00:01:55,090 --> 00:02:03,520
import sensor flow as T.F. then the next thing we're going to import is that fully connected layer will

31
00:02:03,520 --> 00:02:15,040
say from tensor flow contrib layers and poor fully connected OK then we're going to fill out the number

32
00:02:15,040 --> 00:02:17,610
of inputs to fit them mentions the data set.

33
00:02:17,650 --> 00:02:22,330
So we have 30 features so that means we're going to have 30 inputs the number of hidden units in this

34
00:02:22,330 --> 00:02:25,660
case we're going to reduce it down to two then mentions So we'll have that out there.

35
00:02:25,660 --> 00:02:30,130
And then it is already set for us the learning rate you can kind of play around of this but we shouldn't

36
00:02:30,130 --> 00:02:32,140
need a very high learning rate.

37
00:02:32,260 --> 00:02:35,820
The fact of the matter is the state is highly separable even in 30 dimensions.

38
00:02:35,950 --> 00:02:38,190
So we should be able to learn on it pretty fast.

39
00:02:39,550 --> 00:02:41,080
Next we have the placeholder.

40
00:02:41,200 --> 00:02:49,780
So we're just passing in our data will say x is equal to T.F. placeholder and then we'll say flow 32

41
00:02:50,450 --> 00:02:53,730
and shape it's going to basically be defined by that batch size.

42
00:02:53,980 --> 00:03:01,000
And then the number of inputs which is 30 features But next we're going to create some layers hidden

43
00:03:01,000 --> 00:03:05,120
layer is equal to a fully connected layer.

44
00:03:05,320 --> 00:03:11,620
So pass an X than the number of hidden units and then for this case we'll say activation function is

45
00:03:11,620 --> 00:03:12,110
done.

46
00:03:13,420 --> 00:03:17,810
And then the outputs are just going to be a fully connected layer.

47
00:03:19,030 --> 00:03:28,210
And also a hidden number of outputs and then finally no activation function Let's go ahead and create

48
00:03:28,240 --> 00:03:36,940
our last function that's going to be T.F. reduce mean use mean.

49
00:03:37,470 --> 00:03:38,760
And then we're going to take the square.

50
00:03:38,770 --> 00:03:40,300
So basically I mean square there

51
00:03:43,760 --> 00:03:49,110
of the outputs minus X OK we ever mean squared error.

52
00:03:49,150 --> 00:03:59,010
Let's create our optimizer will use an Adam Adam optimizer T.F. train call atom optimized for this person

53
00:03:59,010 --> 00:04:05,310
the learning rate that we defined earlier and then say or trainer operation is just the optimizer trying

54
00:04:05,310 --> 00:04:09,710
to minimize the loss.

55
00:04:09,710 --> 00:04:12,830
Next we're going to create an instance of that global variable initialiser.

56
00:04:12,890 --> 00:04:18,040
So we'll say it is equal to T.F. global variables initializer.

57
00:04:18,230 --> 00:04:21,980
And then we actually create instance of it so we'll make sure we run that and then we actually want

58
00:04:21,980 --> 00:04:23,140
to run the session.

59
00:04:23,150 --> 00:04:27,480
So we're going to create a tensor for session that runs the optimizer for at least a thousand steps.

60
00:04:27,680 --> 00:04:33,690
So you can use ebox if you prefer one ypocras just defined by one single run through the entire dataset.

61
00:04:34,860 --> 00:04:40,000
Well St number of steps is equal to 1000.

62
00:04:40,020 --> 00:04:42,950
Again you could have said something like just run it for two epochs.

63
00:04:43,130 --> 00:04:44,860
Let's make sure that's not connected.

64
00:04:44,940 --> 00:04:56,020
So we'll say it with a session as SPSS session run and it's and then it is safe for every iteration

65
00:04:56,090 --> 00:04:56,850
in the range.

66
00:04:56,850 --> 00:05:04,410
The number of steps we defined earlier thousand steps we're going to say session run train and then

67
00:05:04,410 --> 00:05:10,590
our feed dictionary here is just going to be our scale data.

68
00:05:10,780 --> 00:05:13,010
So we'll go ahead and run that should be pretty fast.

69
00:05:13,030 --> 00:05:18,660
And now we want to create a session that calculates the scaled data going through that hidden layer.

70
00:05:18,670 --> 00:05:27,480
So with T.F. session as SPSS and this is one of those instances where you might want to run a interactive

71
00:05:27,480 --> 00:05:35,600
session with initialiser variables again and then say that the two the output is equal to that hidden

72
00:05:35,600 --> 00:05:45,730
layer and then it evaluates given a dictionary X and then the scale data OK.

73
00:05:45,740 --> 00:05:49,890
So we're going to confirm now that our output is actually the output we expected some to them actual

74
00:05:49,910 --> 00:05:50,590
output.

75
00:05:50,990 --> 00:05:52,650
So we'll check out the shape.

76
00:05:52,730 --> 00:05:58,230
So now it has 500 entries but there's only two features so let's plot it out and see how well we did.

77
00:05:58,230 --> 00:06:04,530
So we'll say Pulte scatter and we want to get output to the I will take.

78
00:06:04,800 --> 00:06:06,890
Well let's just show you output to the it looks like.

79
00:06:06,930 --> 00:06:13,630
So you can kind of get an idea I now have just two columns worth of data from the 30 columns.

80
00:06:13,630 --> 00:06:21,320
So then in going I say grab all the rows in the first column and plot them against all the rows in the

81
00:06:21,320 --> 00:06:23,000
second column.

82
00:06:23,000 --> 00:06:27,200
Now if you just run that and you get something that looks like this where there's not very clear separation

83
00:06:27,620 --> 00:06:33,230
and you can determine that by asking for the color will say color is just defined by that previous label

84
00:06:33,230 --> 00:06:35,120
column.

85
00:06:35,140 --> 00:06:38,130
So we see here they're not really well separated still.

86
00:06:38,220 --> 00:06:41,290
What we can do is we'll come back up here and train it for a couple of more steps.

87
00:06:41,290 --> 00:06:43,180
Let's go ahead and just say 2000 steps.

88
00:06:43,480 --> 00:06:48,400
So we'll run these cells again evaluate again if that new model run the output and then let's run this

89
00:06:48,400 --> 00:06:48,820
again.

90
00:06:48,820 --> 00:06:51,230
So you can see we're getting better or better separation.

91
00:06:51,250 --> 00:06:56,900
So let's run that for just let's say two overkilled or 5000 steps.

92
00:06:56,980 --> 00:07:00,130
Hopefully it doesn't run forever.

93
00:07:00,140 --> 00:07:01,940
Ok it looks like it ran pretty good.

94
00:07:01,940 --> 00:07:02,950
And here we can see.

95
00:07:02,990 --> 00:07:04,860
So now we going getting clear separation.

96
00:07:04,880 --> 00:07:09,050
So if you don't get clear separation at the very beginning you may just have to run it for some more

97
00:07:09,050 --> 00:07:10,530
steps.

98
00:07:10,530 --> 00:07:15,270
Now if we scroll back up here at the instructions that call for a thousand steps that may again not

99
00:07:15,270 --> 00:07:15,870
be enough for you.

100
00:07:15,870 --> 00:07:20,840
So go ahead and just you know change it to some higher number Fermanagh and again at 1000 this time

101
00:07:20,850 --> 00:07:24,320
you can see I kind of get lucky here and I do get that full separation.

102
00:07:24,330 --> 00:07:28,210
So again the more steps you do the more likely you get a higher separation there.

103
00:07:28,440 --> 00:07:32,410
If after many many thousands of steps you're still not getting separation.

104
00:07:32,490 --> 00:07:37,200
Go ahead and run the solution is no book and double check that your all your codes actually matching

105
00:07:37,200 --> 00:07:38,740
the solutions and the solutions.

106
00:07:38,740 --> 00:07:41,550
No but those have a thousand steps as a number of steps there.

107
00:07:41,580 --> 00:07:43,980
Go ahead and amp that up and then check again.

108
00:07:44,070 --> 00:07:44,410
OK.

109
00:07:44,430 --> 00:07:47,070
If you have any questions on this feel free to post that on the forums.

110
00:07:47,070 --> 00:07:48,930
But hopefully it was pretty straightforward.

111
00:07:48,930 --> 00:07:50,690
Thanks everyone and I will see at the next lecture.

