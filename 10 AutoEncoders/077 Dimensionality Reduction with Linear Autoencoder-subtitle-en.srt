1
00:00:05,510 --> 00:00:09,650
Welcome everyone to this lecture Well we show you how we can perform them and anality reduction with

2
00:00:09,650 --> 00:00:16,210
a linear auto encoder linear auto encoders can be used to perform what's known as a principal component

3
00:00:16,270 --> 00:00:20,550
analysis basically allowing us to reduce the dimensionality of our data.

4
00:00:21,550 --> 00:00:27,920
Dimensionality reduction allows us to get a lower mentioned representation of our data and the encoder.

5
00:00:27,930 --> 00:00:34,230
Half of the auto encoder creates new and fewer features from the input features.

6
00:00:34,240 --> 00:00:39,190
So for example we can take a three dimensional dataset an output a two dimensional representation of

7
00:00:39,190 --> 00:00:39,530
it.

8
00:00:39,730 --> 00:00:44,380
So we have an input layer with three neurons and a hidden layer of just two neurons.

9
00:00:44,380 --> 00:00:50,620
Now keep in mind we're not just simply choosing two of the original three features but instead we're

10
00:00:50,620 --> 00:00:56,110
constructing two new features from combinations of the original three and using those combinations we

11
00:00:56,110 --> 00:01:03,220
can then reduce the actual damage anality of the original data set and we achieve this effect by using

12
00:01:03,220 --> 00:01:08,680
a linear auto encoder and a linear auto encoders perform the linear transformations by only using the

13
00:01:08,680 --> 00:01:09,890
weights and biased terms.

14
00:01:09,910 --> 00:01:13,320
We actually don't use activation function.

15
00:01:13,430 --> 00:01:16,500
So essentially we have an auto encoder that looks like this.

16
00:01:16,580 --> 00:01:21,290
It's the exact same thing that we saw earlier in the previous lecture just know that we're not passing

17
00:01:21,350 --> 00:01:23,000
an activation function anymore.

18
00:01:23,090 --> 00:01:29,160
We're just using the linear transformation of the input and the hidden output.

19
00:01:29,180 --> 00:01:35,150
Now after training what we do is we use the hidden layer to obtain a reduced dimensionality.

20
00:01:35,170 --> 00:01:38,110
So let's go ahead and code this out to see how it works.

21
00:01:38,110 --> 00:01:43,720
If you want more substantial mathematical proof of why the linear auto encoder can perform this principle

22
00:01:43,720 --> 00:01:46,760
component analysis go ahead and check out the resource links.

23
00:01:46,870 --> 00:01:51,880
Some of the geometric notation is a little bit outside the scope of this course but you can go ahead

24
00:01:51,880 --> 00:01:56,080
and check out the link in case you want full mathematical proof of why this works.

25
00:01:56,080 --> 00:02:00,160
But let's not jump to the Jupiter note book and actually implement this.

26
00:02:00,220 --> 00:02:04,780
All right so here I am at a Jupiter notebook and the first thing I want to start off by doing is a couple

27
00:02:04,780 --> 00:02:09,440
of imports will Important them pie and then I also want to do a little bit of visualization.

28
00:02:09,490 --> 00:02:16,160
So we'll import matplotlib that pipeline as Piazzi as.

29
00:02:16,300 --> 00:02:22,320
And then I also need to indicate that live in line since I'm using the Jupiter notebook.

30
00:02:22,480 --> 00:02:29,140
So now I'm going to show you how we can easily create some data sets so you can say from S-K learn dot

31
00:02:29,310 --> 00:02:31,360
datasets import.

32
00:02:31,670 --> 00:02:37,600
And this is a really nice make Blob's functionality and make Blob's what it does is it basically creates

33
00:02:37,600 --> 00:02:44,510
classification data sets for you so you can say data is equal to and then call make Blob's.

34
00:02:44,700 --> 00:02:47,840
And if you do shift type here you can see what it actually does.

35
00:02:47,860 --> 00:02:52,840
It takes in the number of samples you want the number of features you want how many centers basically

36
00:02:52,840 --> 00:02:58,000
how many classes you want and then the cluster standard deviation so the standard deviation between

37
00:02:58,000 --> 00:03:03,370
these actual clusters and then you can also specify other things such as the random state.

38
00:03:03,370 --> 00:03:08,750
So I'm going to create a very simple example will say we'll have 100 samples.

39
00:03:08,770 --> 00:03:10,720
So that is 100 rows of data.

40
00:03:11,260 --> 00:03:14,860
The number of features we want is going to be three features.

41
00:03:14,860 --> 00:03:17,330
So we're basically creating a three dimensional data set.

42
00:03:17,470 --> 00:03:22,650
And this is the data set we're going to try to reduce into to them and Chinse and then we'll say centers.

43
00:03:22,690 --> 00:03:23,440
Just too.

44
00:03:23,440 --> 00:03:26,890
So this is going to be basically two blobs or two classes.

45
00:03:27,220 --> 00:03:28,420
And then here's the important part.

46
00:03:28,420 --> 00:03:30,630
Make sure you have the same random state as me.

47
00:03:30,850 --> 00:03:32,550
Otherwise you'll get different data set.

48
00:03:32,590 --> 00:03:34,620
So that basically sets were in them seed.

49
00:03:35,080 --> 00:03:40,740
OK so now that we have our data if we take a look at what type is actually return it's a tuple.

50
00:03:40,750 --> 00:03:43,710
And if you print all the data you end up seeing something like this.

51
00:03:43,870 --> 00:03:44,920
So you have a large array.

52
00:03:44,950 --> 00:03:48,450
Those are all your values so here's 100 samples with three features.

53
00:03:48,460 --> 00:03:52,300
Those are the columns and then you scroll all the way down you end up seeing the actual classes they

54
00:03:52,300 --> 00:03:53,320
belong to.

55
00:03:53,320 --> 00:03:57,760
Now keep in mind we're not actually performing any sort of classification task right now but the reason

56
00:03:57,760 --> 00:04:03,400
I like having these classes for walking you through PCa or it dimensionality reduction is because it

57
00:04:03,400 --> 00:04:07,850
makes it really clear that you're still able to retain a lot of information.

58
00:04:07,900 --> 00:04:10,000
And so what I mean by that is the following.

59
00:04:10,000 --> 00:04:12,640
So first off let's actually grab some of this data.

60
00:04:13,120 --> 00:04:18,020
So if we say data of 0 then this is just our actual data.

61
00:04:18,070 --> 00:04:21,030
If we say Data 1 then these are our labels.

62
00:04:21,310 --> 00:04:29,700
So we pass this in we're going to need to do some scaling which means we'll say from Escuela and pre-processing

63
00:04:30,300 --> 00:04:37,940
import that min max scaler were already familiar with I will say scaler minimax scaler and then we'll

64
00:04:37,950 --> 00:04:43,170
report back scaled data is equal to scaler transform

65
00:04:46,210 --> 00:04:46,970
and notice here.

66
00:04:47,020 --> 00:04:49,980
I'm going to just transform my entire dataset.

67
00:04:50,050 --> 00:04:53,530
That's because we're doing what is essentially an unsupervised task.

68
00:04:53,530 --> 00:04:56,540
So I don't really care about any sort of train split.

69
00:04:56,560 --> 00:05:01,610
I'm just going to take all my data that's three dimensional and try to convert it into two dimensions.

70
00:05:01,690 --> 00:05:03,940
So do a dimensionality reduction.

71
00:05:04,030 --> 00:05:11,110
But before I do that I need actually scale it sonand I have my data that's scaled and I'm going to basically

72
00:05:11,110 --> 00:05:12,610
plot it out for you.

73
00:05:13,120 --> 00:05:15,240
So let's grab it feature by feature.

74
00:05:15,700 --> 00:05:22,780
We'll say scaled data all the rows in the zero column I will say that's data X and then we'll do this

75
00:05:23,050 --> 00:05:24,190
three more times.

76
00:05:24,490 --> 00:05:27,750
Except for the other two columns.

77
00:05:27,770 --> 00:05:32,780
So what I'm doing here is I'm just grabbing the columns and then set in them as variables data x y and

78
00:05:32,990 --> 00:05:38,260
z so I scaled my data and I grab the three individual columns.

79
00:05:38,260 --> 00:05:41,500
And now what I can do is I can plot them in three dimensions.

80
00:05:41,530 --> 00:05:50,950
So the way we do that with MAP plot lib you say from NPL underscore toolkits dots and plot three the

81
00:05:51,550 --> 00:06:00,430
import axes 3D and this allows you to create some kind of rudimentary three dimensional plot and the

82
00:06:00,430 --> 00:06:03,280
way you do this is if you say fig peel.

83
00:06:03,330 --> 00:06:05,500
See that figure.

84
00:06:05,510 --> 00:06:09,170
So you create a figure object and then you get an access to this.

85
00:06:09,400 --> 00:06:13,120
And the worry about understand this too much because this is really just for the visualization of the

86
00:06:13,120 --> 00:06:19,260
problem it's not a huge part of this course you passing one on one to basically in the case that you're

87
00:06:19,270 --> 00:06:27,800
just doing one plot at the very first grade and then you need to specify a projection is equal to 3D.

88
00:06:28,190 --> 00:06:31,750
So if you run that you get this kind of 3D looking plot.

89
00:06:31,760 --> 00:06:37,310
Keep in mind map plot lib is not specifically designed to be a super great three dimensional visualization

90
00:06:37,310 --> 00:06:37,940
library.

91
00:06:38,030 --> 00:06:42,620
It does have a lot of useful tools for you but if you're really interested in a three dimensional plotting

92
00:06:42,620 --> 00:06:47,230
with Python I was just going for interactive library like Bocca or plot.

93
00:06:47,230 --> 00:06:51,560
Lee again we don't really cover these topics in this course because it's not a visualisation course

94
00:06:51,860 --> 00:06:53,400
but I just want to keep that in mind.

95
00:06:53,420 --> 00:06:57,740
This is not Matt Hotlips greatest strength but it does have the capabilities for it.

96
00:06:57,740 --> 00:07:01,970
So the reason that I want to show you this is because this is what we're going to plot on.

97
00:07:01,970 --> 00:07:08,270
So on this axis I'm then going to say X and call a scatterplot on it just like you would normally say

98
00:07:08,270 --> 00:07:09,360
appeal to scatter.

99
00:07:09,440 --> 00:07:17,990
Except now what's special about it is I can define X Y and Z which means I'll just pasan those columns

100
00:07:17,990 --> 00:07:18,390
of data

101
00:07:21,520 --> 00:07:21,960
OK.

102
00:07:21,980 --> 00:07:22,640
There we have it.

103
00:07:22,640 --> 00:07:26,090
So now I can see my two blobs in three dimensional space.

104
00:07:26,210 --> 00:07:32,090
So clearly when I plotted out in three dimensions they're very separable and you could basically put

105
00:07:32,090 --> 00:07:37,670
in you can imagine like a theoretical piece of paper somewhere here and easily separate those two classes.

106
00:07:37,850 --> 00:07:44,330
And we can actually pass in a third argument or a fourth argument really see and then say this is data

107
00:07:44,330 --> 00:07:47,550
one those labels and then I can color it.

108
00:07:47,600 --> 00:07:54,020
And now we're going to attempt to basically answer is given that we have three dimensions worth of data

109
00:07:54,500 --> 00:07:58,940
and with three dimensions I can clearly see that this data is highly separable.

110
00:07:58,940 --> 00:08:01,120
The two blobs are completely separate.

111
00:08:01,130 --> 00:08:07,030
If I try to use a linear auto encoder to reduce these three dimensions to just two them mentions will

112
00:08:07,050 --> 00:08:12,230
I still have that same level of separation and it should be obvious because I will plot it out just

113
00:08:12,230 --> 00:08:16,320
on two axes and then see if I color them by their specific labels.

114
00:08:16,340 --> 00:08:21,830
If I still get that same separation or if by reducing through them inches into to them inches now I

115
00:08:21,830 --> 00:08:22,960
have quite a bit of.

116
00:08:23,000 --> 00:08:25,240
So to speak cross-contamination.

117
00:08:25,490 --> 00:08:30,740
Let's go ahead and explore this idea and see if we can successfully reduce these three dimensions of

118
00:08:30,770 --> 00:08:35,670
data by using combinations for linear auto encoder into to them engines of data.

119
00:08:35,720 --> 00:08:41,870
Again just to be very clear I'm not just throwing away one column and then having two columns.

120
00:08:41,900 --> 00:08:46,760
I'm going to try to make two new dimensions out of these three old ones and that's where the linear

121
00:08:46,820 --> 00:08:48,650
auto encoder comes into play.

122
00:08:48,650 --> 00:08:49,760
So let's go ahead and do this.

123
00:08:49,760 --> 00:08:58,400
We'll say sensor flow as T.F. and then we're going to be using a higher level a Pine-Sol say from Florida

124
00:08:58,430 --> 00:09:01,850
courtroom the layers in poor.

125
00:09:02,110 --> 00:09:06,250
And if hit tab here was being hit it's because I haven't imported it yet.

126
00:09:06,470 --> 00:09:08,590
You should be able to say fully connected.

127
00:09:08,690 --> 00:09:09,320
What's that.

128
00:09:09,350 --> 00:09:10,120
Oh OK.

129
00:09:10,160 --> 00:09:12,070
Now the tab shows up a little too late.

130
00:09:12,080 --> 00:09:15,570
They can see here there's a bunch of letters that are already created for you.

131
00:09:15,620 --> 00:09:19,540
The one we're going to be using is a very simple fully connected layer just like we saw in our diagrams

132
00:09:19,940 --> 00:09:25,980
and now we're going to do is create a couple of variables we'll see the number of inputs is equal to

133
00:09:25,980 --> 00:09:28,840
3 because we have three dimensions for data.

134
00:09:28,890 --> 00:09:33,980
We'll have our hidden layer be equal to two mentions of data.

135
00:09:34,160 --> 00:09:38,990
And then finally remember because we're using an auto encoder the number of outputs needs to be equal

136
00:09:38,990 --> 00:09:42,360
to the number of inputs.

137
00:09:42,610 --> 00:09:46,900
And while we're at it if we're defining constants we may as well define or learning rate.

138
00:09:47,050 --> 00:09:53,270
Well go ahead and set this to 0.01 we don't have a lot of data so we don't have to learn that slowly.

139
00:09:53,290 --> 00:09:58,360
Up next we're going to create a placeholder so there's only one placeholder because unsupervised hask

140
00:09:58,360 --> 00:10:00,140
we're not dealing with any labels.

141
00:10:00,440 --> 00:10:05,200
So I'll say placeholder to float 32.

142
00:10:05,450 --> 00:10:10,470
And the shape is going to be defined by whatever a batch size we feed so none.

143
00:10:10,550 --> 00:10:16,540
And then by the number of them puts and then let's go ahead and create our layers using this contrib

144
00:10:16,710 --> 00:10:17,300
layers.

145
00:10:17,320 --> 00:10:26,650
So we'll say it in is equal to a fully connected layer and a fully connected layer needs some inputs

146
00:10:26,950 --> 00:10:30,410
number of outputs and activation function and it can take more than that.

147
00:10:30,640 --> 00:10:39,250
But we're going to pass an X as the input the output is going to be the number of hidden units and then

148
00:10:39,250 --> 00:10:43,610
we have our activation function and this is where we're actually going to say none.

149
00:10:43,750 --> 00:10:48,850
Because remember with that linear auto encoder just as we showed in the slides we are actually passing

150
00:10:48,850 --> 00:10:53,740
this with an activation function that we have or outputs layer.

151
00:10:53,890 --> 00:11:00,780
And again this is going to be fully connected layer and that's going to take in that hidden layer.

152
00:11:00,960 --> 00:11:06,300
It's going to have the number of outputs it's going to spit it back out to three and then again no activation

153
00:11:06,300 --> 00:11:07,820
function.

154
00:11:07,840 --> 00:11:14,830
So all this is trying to do is successfully reproduce the input at the output but there's a step in

155
00:11:14,830 --> 00:11:19,610
between where it reduces the number of dimensions OK.

156
00:11:19,630 --> 00:11:21,830
Now we have hidden and outputs.

157
00:11:21,850 --> 00:11:22,990
So let's do the rest of it.

158
00:11:22,990 --> 00:11:29,370
We need the last function here will say loss is equal to T.F. reduce mean.

159
00:11:29,640 --> 00:11:38,480
This is just a mean square air T.F. square and then we'll say let's make sure it's all square right.

160
00:11:42,160 --> 00:11:50,210
And we'll say the outputs minus X because remember X is the actual input and then finally we need an

161
00:11:50,210 --> 00:11:59,050
optimizer will say optimizer is equal to T.F. train and we'll use an atom optimizer and I will specify

162
00:11:59,050 --> 00:11:59,920
some learning rate.

163
00:12:01,530 --> 00:12:05,520
And they will say train is equal to optimizer and we're trying to minimize that loss function

164
00:12:08,520 --> 00:12:09,000
OK.

165
00:12:09,060 --> 00:12:15,640
Almost there except we need to initialize a variable so say if global variables initialiser.

166
00:12:16,000 --> 00:12:20,710
Now it's time to actually run the session and see if we're able to successfully reduce this from three

167
00:12:20,710 --> 00:12:26,560
dimensions into two them mentions and still retain information and we'll say will basically clarify

168
00:12:26,560 --> 00:12:33,330
that we retain the information if we can still see that these two classes are highly separable Let's

169
00:12:33,340 --> 00:12:38,550
go ahead and run this for a thousand steps we'll say with T.F. session.

170
00:12:38,590 --> 00:12:46,840
As s s s say SPSS run that initialiser first and then we're going to do the following.

171
00:12:47,090 --> 00:12:52,410
We'll say for iteration in the number of steps we're taking.

172
00:12:52,770 --> 00:13:03,600
So 1000 steps we're going to run train and then pass in our feet dictionary surfie dictionary is just

173
00:13:03,600 --> 00:13:06,800
going to be X and we're going to pass in the scale data from before.

174
00:13:06,810 --> 00:13:09,630
So that's all three dimensions the features.

175
00:13:09,810 --> 00:13:12,950
And then once that's done training I'm going to do the following.

176
00:13:12,960 --> 00:13:18,410
I'm going to say my to that mentioner output is equal to the hidden layer.

177
00:13:18,900 --> 00:13:21,980
And then I'm going to evaluate the same feed dictionary.

178
00:13:21,990 --> 00:13:26,620
So I'll say feed dictionary X skilled data.

179
00:13:26,670 --> 00:13:29,030
So let's explain what we're actually doing here.

180
00:13:29,070 --> 00:13:35,700
I'm going to train my auto encoder for 1000 steps and I need to keep passing that scale data and this

181
00:13:35,730 --> 00:13:41,790
auto encoder all is trying to do is it takes and the inputs shrinks them down to just two neurons and

182
00:13:41,790 --> 00:13:47,190
then expands them back out to three and then the way it's actually being optimized is by trying to make

183
00:13:47,190 --> 00:13:51,120
sure its outputs match the inputs as closely as possible.

184
00:13:51,360 --> 00:13:56,560
And the challenges this hidden layer has to reduce from three dimensions into two them engines.

185
00:13:56,580 --> 00:13:59,620
Now given this actual data set it's not a very hard task.

186
00:13:59,670 --> 00:14:03,780
You could see that they were highly separable and the stonecutter is going to perform very well as you'll

187
00:14:03,780 --> 00:14:04,740
see in just a second.

188
00:14:04,890 --> 00:14:09,530
But keep in mind we'll still be able to apply the same practice for something with 30 dimensions and

189
00:14:09,540 --> 00:14:15,240
maybe reduce it down to three dimensions but all we're doing is trying to get that input passers through

190
00:14:15,240 --> 00:14:20,340
the hidden them put it through the outputs see how far we are from the outputs the original data source

191
00:14:20,730 --> 00:14:24,870
and then once we've done that through our training we're going to do is after these 1000 training steps.

192
00:14:24,870 --> 00:14:26,460
Notice this is outside the for loop.

193
00:14:26,670 --> 00:14:33,650
I'm going to say hey now taken my skill data and only pass it through that hidden layer.

194
00:14:33,660 --> 00:14:36,540
That way I get to them and output backout.

195
00:14:36,780 --> 00:14:41,340
And if you want to you could perform this in a separate session so it'll show how it looks in just a

196
00:14:41,340 --> 00:14:42,040
second.

197
00:14:42,330 --> 00:14:44,310
So I just ran that for a thousand training steps.

198
00:14:44,340 --> 00:14:45,500
It should be pretty fast.

199
00:14:45,690 --> 00:14:52,920
And now I have output to the end if you take a look at it and it's now the two that mentioned all representation

200
00:14:53,220 --> 00:14:55,560
of our original scale data source.

201
00:14:55,590 --> 00:14:57,070
So let's go ahead and plot this out.

202
00:14:57,180 --> 00:15:09,670
We'll say t scatter and I'm going to plot out the first column of data versus the second column of data.

203
00:15:13,930 --> 00:15:14,800
And we plot that out.

204
00:15:14,800 --> 00:15:16,130
We get two blobs.

205
00:15:16,150 --> 00:15:19,770
Now we still don't know whether we're separating these versus.

206
00:15:19,770 --> 00:15:25,620
Let's go and add a little bit of color in and hopefully these to match up with their respective classes.

207
00:15:25,780 --> 00:15:31,080
And now we can see that we were successfully able to reduce the number of mentions from three mentions

208
00:15:31,090 --> 00:15:37,390
to two that mentions the challenge in this interpretation of the results comes from trying to understand

209
00:15:37,720 --> 00:15:40,040
what these new dimensions actually represent.

210
00:15:40,180 --> 00:15:46,810
And unfortunately you can't just directly say hey this x axis relates to the first feature in our original

211
00:15:46,810 --> 00:15:47,620
dataset.

212
00:15:47,710 --> 00:15:53,410
There is definitely some relationship there but it's hard to interpret because it depends on the actual

213
00:15:53,410 --> 00:15:56,800
weights of that hidden layer in a simple auto encoder.

214
00:15:56,800 --> 00:16:02,210
You could definitely go back and actually look at the weights and see that hey maybe it's 60 percent

215
00:16:02,220 --> 00:16:06,470
a feature 1 30 percent a feature to 20 percent feature three etc..

216
00:16:06,670 --> 00:16:11,800
But as we get larger and larger with our inputs that interpretation is going to become harder and harder

217
00:16:11,920 --> 00:16:16,570
and then especially when we get to stack auto encoders where we don't just have one hidden layer is

218
00:16:16,630 --> 00:16:21,550
that we have multiple hidden layers actually interpreting the combinations is essentially going to be

219
00:16:21,790 --> 00:16:22,740
like a black box.

220
00:16:22,750 --> 00:16:24,970
You're not going to directly interpret that.

221
00:16:25,020 --> 00:16:28,950
Keep in mind that's also a challenge which is normal principal component analysis.

222
00:16:28,990 --> 00:16:34,920
So you're not really sacrificing interpretability in that sense by using a linear auto encoder.

223
00:16:35,110 --> 00:16:36,100
So keep that in mind.

224
00:16:36,130 --> 00:16:41,290
That's a normal principal component and also still suffers from that same interpretation problem that

225
00:16:41,290 --> 00:16:46,850
it's hard to just get an intuitive sense of what access won here and access to represents.

226
00:16:46,930 --> 00:16:51,820
However the most important part is we can clearly see even on a visual stance it still works.

227
00:16:51,820 --> 00:16:57,640
We were able to successfully reduce the number of dimensions in this dataset and still retain information

228
00:16:57,910 --> 00:17:01,710
or we're finding information as the separation of classes here.

229
00:17:02,020 --> 00:17:02,620
OK.

230
00:17:02,950 --> 00:17:08,380
So up next you're going to be running this same exercise except you're going to be doing it on a set

231
00:17:08,380 --> 00:17:13,990
of data that has 30 dimensions or features and hopefully you'll be able to see that even with 30 dimensions

232
00:17:14,050 --> 00:17:19,040
you'll be able to successfully reduce it back down to just two them engines and retain that super ability

233
00:17:19,330 --> 00:17:20,360
of data.

234
00:17:20,380 --> 00:17:24,670
Thanks everyone and I'll see you at the next lecture where we'll cover over the exercise.

